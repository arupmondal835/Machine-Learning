{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# Project 2: First approaches to MultiModal Transformers: Bridging Text with Vision, Audio, and Video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abstract",
   "metadata": {},
   "source": [
    "# Objective: \n",
    "\n",
    "Instead of treating text, audio, and video as separate streams of information, you will design a **Transformer-based model that intelligently fuses two modalities**—text with images, text with audio, or text with video. Your challenge is to harness the power of deep learning to create a system where each modality enhances the other, unlocking richer, more meaningful insights.\n",
    "\n",
    "This is more than just training a model—it’s about innovation. How will you design a fusion strategy that truly captures cross-modal relationships? Will your model generate creative text from images, answer questions from audio, or retrieve videos based on descriptions? The decisions are yours to make.\n",
    "\n",
    "Even if you and your peers work with similar datasets, your approach must be unique. Whether through data choices, architectural modifications, or fusion techniques, your model should push the boundaries of multimodal AI. Experiment boldly, optimize strategically, and most importantly—create something exciting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22115c68",
   "metadata": {},
   "source": [
    "# Deliverables:\n",
    "\n",
    "- A working model (hybrid architecture)\n",
    "- A structured report (including visuals & reflections) **Required: Include details about your hybrid architecture!!!!**\n",
    "- A GitHub repository with clean, documented code\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c0842f",
   "metadata": {},
   "source": [
    "# Step 1: Select your own adventure\n",
    "\n",
    "Below is a concise, high-level breakdown of three main multimodal “adventures,” each with several task options and brief notes about potential datasets and implementation tips. This structure makes it easy to pick a project that best fits your interests and available resources—whether you prefer images, audio, or video combined with text. **Required in your reports regardless of the choice picked: Include details about your hybrid architecture!!!!**\n",
    "\n",
    "## **Choice 1: Images + Text**\n",
    "\n",
    "### 1. **Image Captioning**\n",
    "- **Goal:** Automatically generate textual descriptions (captions) for given images.  \n",
    "- **Potential Datasets:**  \n",
    "  - **MS COCO** – Large-scale, ~330k images with multiple captions per image.  \n",
    "  - **Flickr8k/30k** – Smaller datasets; useful for quick iteration.  \n",
    "- **Implementation Tips:**  \n",
    "  - Use a **CNN or Vision Transformer** to encode images, then a Transformer decoder for generating text.  \n",
    "  - Evaluate output with **BLEU, METEOR, or CIDEr**.\n",
    "\n",
    "### 2. **Visual Question Answering (VQA)**\n",
    "- **Goal:** Answer open-ended questions about image content (e.g., “How many dogs are in this picture?”).  \n",
    "- **Potential Datasets:**  \n",
    "  - **VQA v2** – 204k images and ~1 million Q&A pairs.  \n",
    "  - **GQA** – Emphasizes compositional reasoning.  \n",
    "- **Implementation Tips:**  \n",
    "  - Fuse **image features** (from a CNN/ViT) with **question embeddings** (Transformer for text).  \n",
    "  - Evaluate with **accuracy** for classification-based answers or **language metrics** for open-ended answers.\n",
    "\n",
    "\n",
    "### 3. **Image-Text Retrieval**\n",
    "- **Goal:** Retrieve the most relevant images given a text query, or vice versa.  \n",
    "- **Potential Datasets:**  \n",
    "  - **MS COCO** – Commonly used for both captioning and retrieval.  \n",
    "  - **Flickr30k** – Includes structures suited to retrieval tasks.  \n",
    "- **Implementation Tips:**  \n",
    "  - Use **dual encoders** for image and text, trained with a **contrastive loss** to align modalities.  \n",
    "  - Evaluate with **Recall@K** or **mean rank** metrics.\n",
    "\n",
    "---\n",
    "\n",
    "## **Choice 2: Audio + Text**\n",
    "\n",
    "### 1. **Speech Recognition**\n",
    "- **Goal:** Convert spoken language (waveforms) into written text transcripts.  \n",
    "- **Potential Datasets:**  \n",
    "  - **LibriSpeech** – ~1,000 hours of English audiobook recordings.  \n",
    "  - **Mozilla Common Voice** – Crowd-sourced, multilingual speech data.  \n",
    "- **Implementation Tips:**  \n",
    "  - Convert waveforms into **Mel spectrograms**, or use **wav2vec2** (pretrained).  \n",
    "  - Evaluate with **Word Error Rate (WER)**.\n",
    "\n",
    "### 2. **Audio-Text Alignment**\n",
    "- **Goal:** Match spoken words or segments in an audio file to their written transcripts (often down to timestamps).  \n",
    "- **Potential Datasets:**  \n",
    "  - **TEDLIUM** – TED talks with aligned transcripts.  \n",
    "  - **YouTube** auto-transcripts (though noisier).  \n",
    "- **Implementation Tips:**  \n",
    "  - Segment audio frames; align with text tokens.  \n",
    "  - Use **CTC-based** approaches or techniques like Dynamic Time Warping (DTW).  \n",
    "  - Applications: **karaoke-style** subtitles, real-time captioning.\n",
    "\n",
    "\n",
    "### 3. **Spoken Command Classification**\n",
    "- **Goal:** Identify short, predefined voice commands like “Turn on the light.”  \n",
    "- **Potential Datasets:**  \n",
    "  - **Google Speech Commands** – Tens of thousands of short utterances for specific commands.  \n",
    "- **Implementation Tips:**  \n",
    "  - A **classification task** (label each audio clip with the intended command).  \n",
    "  - Evaluate with **accuracy** or **F1 score**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Choice 3: Video + Text**\n",
    "\n",
    "### 1. **Video Captioning**\n",
    "- **Goal:** Generate textual descriptions for short videos (e.g., “A person cooking pasta”).  \n",
    "- **Potential Datasets:**  \n",
    "  - **MSR-VTT** – ~10k short video clips, each with multiple captions.  \n",
    "  - **YouCook2** – Cooking videos with detailed instructions.  \n",
    "- **Implementation Tips:**  \n",
    "  - Sample frames (e.g., 1 fps) for each video.  \n",
    "  - Encode frames (CNN/ViT) and use a Transformer decoder for text.  \n",
    "  - Evaluate with **BLEU, METEOR, or CIDEr**.\n",
    "\n",
    "\n",
    "### 2. **Video Question Answering (Video QA)**\n",
    "- **Goal:** Answer questions based on video content (objects, actions, context).  \n",
    "- **Potential Datasets:**  \n",
    "  - **TVQA** – TV show clips plus questions about dialogue and visuals.  \n",
    "  - **LSMDC** – Movie clips with descriptions/questions.  \n",
    "- **Implementation Tips:**  \n",
    "  - Extract **visual features** from sampled frames; optionally include **subtitles/transcripts**.  \n",
    "  - Fuse them with question embeddings in a multimodal Transformer.  \n",
    "  - Evaluate with **accuracy** or open-ended **language metrics**.\n",
    "\n",
    "\n",
    "### 3. **Text-Based Video Retrieval**\n",
    "- **Goal:** Find relevant video clips from a database based on a text query (e.g., “Videos of someone playing guitar”).  \n",
    "- **Potential Datasets:**  \n",
    "  - **MSR-VTT** – Contains clips plus textual metadata.  \n",
    "  - **ActivityNet Captions** – Videos with temporal captions.  \n",
    "- **Implementation Tips:**  \n",
    "  - Use **dual encoders** or a **joint embedding** space.  \n",
    "  - Evaluate with **Recall@K**, **MRR** (Mean Reciprocal Rank), or similar retrieval metrics.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9ba828-2616-4525-a8ca-5c85fe2f57db",
   "metadata": {},
   "source": [
    "## **General Reccomendations**\n",
    "\n",
    "\n",
    "#### 1. **Transformer Architecture**\n",
    "\n",
    "- **Separate Encoders:** Build one encoder for text and another for your chosen modality. Fuse the resulting embeddings either through cross-attention or by concatenating them, then feeding them into further layers.\n",
    "\n",
    "- **Learned Modality Embeddings:** Introduce special learned tokens (e.g., [IMAGE], [AUDIO], [VIDEO]) to flag which modality a token or embedding belongs to. This can help the Transformer distinguish between, say, a text token vs. an image patch embedding.\n",
    "\n",
    "- **Cross-Attention:** If you’re using an encoder–decoder structure (common for generation tasks like captioning), the decoder can attend to both text representations and other modality representations. This is especially potent if your final output is text (e.g., describing an image or transcribing an audio snippet).\n",
    "\n",
    "- **Positional or Spatial Embeddings:**\n",
    "Images/Videos: 2D positional embeddings to capture spatial layout.\n",
    "Audio: Time–frequency positional embeddings to reflect temporal progression.\n",
    "Text: Standard 1D positional embeddings or relative positioning can suffice.\n",
    "\n",
    "#### 2. **Fusion Strategy for Multimodality**\n",
    "\n",
    "- **Concatenation:** The simplest method—just stack text embeddings and modality embeddings along the sequence dimension. Make sure each chunk has a clear positional signal.\n",
    "\n",
    "- **Attention-based Fusion:**\n",
    "Let each modality have its own encoder.\n",
    "Combine them via cross-attention in later layers, where the text representation attends to the image/audio/video representation or vice versa.\n",
    "You might even try mutual cross-attention for an even richer representation.\n",
    "\n",
    "- **Late Fusion**: Encode each modality separately, then merge the final embeddings (e.g., by averaging, concatenation, or a learnable projection) to feed into a classification or decoding head.\n",
    "\n",
    "#### 3. **Training Loop and Objective**\n",
    "\n",
    "- **Loss Functions**\n",
    "Text Generation (e.g., captioning): Cross-entropy on the predicted tokens.\n",
    "Classification (VQA, spoken command classification): Cross-entropy or binary cross-entropy.\n",
    "Retrieval (matching text to images/ audio/video): Contrastive or triplet loss.\n",
    "\n",
    "- **Masking**\n",
    "Carefully handle [PAD] tokens so the attention mechanism ignores those placeholders. Use key padding masks in PyTorch for both the source and target.\n",
    "\n",
    "- **Training Details**\n",
    "Use AdamW or a similar optimizer with a suitable learning rate scheduler (e.g., warmup + decay).\n",
    "Watch your GPU memory usage. If your model or data is large, consider gradient checkpointing or reduce batch size.\n",
    "\n",
    "---\n",
    "\n",
    "## **Summary**\n",
    "Each **Choice** (Images + Text, Audio + Text, or Video + Text) comes with **three distinct tasks** of escalating complexity. Select the modality and task that excite you most and that fit your available computing resources. Focus on building a solid **data pipeline**, leveraging **pretrained models**, and performing **continuous evaluation** to ensure tangible progress over your project timeline. \n",
    "\n",
    "### **You will need to research some of the approaches reccomended here, but, believe me, that is the way real world works! Frustration is always allowed!**\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e22e37",
   "metadata": {},
   "source": [
    "## Clarification\n",
    "\n",
    "You **don't** need to develop an interactive application for this project. The demo will serve as a platform to communicate your results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085a5204",
   "metadata": {},
   "source": [
    "# Step 2: Submit Your Work\n",
    "\n",
    "Your submission package should include:\n",
    "\n",
    "1. **GitHub Repository** (Well-documented code). ``add`` and ``commit`` the final version of your work, and ``push`` your code to your GitHub repository. You can have multiple notebooks. It is up to you.\n",
    "2. **Project Report** – 4-page IEEE-format paper. Write a paper with no more than 4 pages addressing the architecture, tasks outcomes and reflections. When writing this report, consider a business-oriented person as your reader (e.g. your PhD advisor, your internship manager, etc.). Tell the story for each datasets' goal and tasks covered. **Required: Include details about your hybrid architecture!!!!** Also, include insights about:\n",
    "- Significance of your implementation\n",
    "- Accuracy, loss curves, feature importance.\n",
    "- What worked, what didn’t, what’s next?\n",
    "- Where could this be applied?\n",
    "\n",
    "3. **Demo Link or Video** (Showcasing your model’s workflow)\n",
    "4. **README.md file.** Edit the readme.md file in your repository and how to use your code. Ensure reproducibility (environment requirements, requirements.txt, or environment.yml)\n",
    "\n",
    "\n",
    "**``Submit the URL of your GitHub Repository as your assignment submission on Canvas.``**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1397196c-a2ac-4104-8573-661a6c1718cc",
   "metadata": {},
   "source": [
    "### I am choosing the first option: Image + Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f534df-9888-4a74-8156-e98bb53b7cb6",
   "metadata": {},
   "source": [
    "#### 1. Image Captioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb1fc19-6254-4d8c-8758-a9427bf7cc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "# os.environ['PYCARET_CUSTOM_LOGGING_LEVEL'] = 'CRITICAL'\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "from PIL import Image\n",
    "# import tensorflow as tf\n",
    "# import keras\n",
    "# from keras import layers\n",
    "# from keras.applications import efficientnet\n",
    "# from keras.layers import TextVectorization\n",
    "# from keras.preprocessing.image import load_img, img_to_array\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from tqdm import tqdm_notebook\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import random\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(device, num_gpus)\n",
    "\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = '0'  # or the ID of the specific GPU you want to use\n",
    "#os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5beb12-7f4b-413d-888b-036072488b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64746288-8d7f-4a5a-8b20-034209f1f72d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ffc892-73e4-4211-8513-2bc6a19227fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe38c7e-90d3-44b9-9baa-25f38170b8fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aebceb6e",
   "metadata": {},
   "source": [
    "#### Let's understand the data by plotting the captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fbe37b-9cc5-4b61-8c2b-d2fbf54204c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV into a pandas DataFrame\n",
    "CAPTIONS_PATH = \"flickr_dataset/flickr30k_images/results.csv\"\n",
    "captions_df = pd.read_csv(CAPTIONS_PATH, on_bad_lines='skip', sep = \"|\")\n",
    "\n",
    "# Display the first few rows to inspect the data\n",
    "print(captions_df.columns)\n",
    "\n",
    "captions_df['comment'] = captions_df[' comment'].fillna('').astype(str)\n",
    "\n",
    "#captions_df['comment_length'] = captions_df['comment'].apply(len)\n",
    "captions_df['comment_length'] = captions_df['comment'].apply(lambda x: len(x.split()))\n",
    "\n",
    "#Plot the distribution of comment lengths\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(captions_df['comment_length'], bins=50, color='skyblue', edgecolor='black')\n",
    "plt.title('Distribution of Comment Lengths')\n",
    "plt.xlabel('Comment Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Convert to lowercase and split on spaces\n",
    "all_words = ' '.join(captions_df['comment'].fillna('')).lower().split()\n",
    "\n",
    "# Get unique words\n",
    "unique_words = set(all_words)\n",
    "\n",
    "# Get the count\n",
    "num_unique_words = len(unique_words)\n",
    "print(f'Number of unique words: {num_unique_words}')\n",
    "\n",
    "for comment in captions_df[captions_df['comment_length'] > 75]['comment']:\n",
    "    print(\"longest captions:\", comment, \"\\n\" + \"-\"*75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdc8a45-2a94-44c2-8c2f-f2466eb6773d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "# import os\n",
    "\n",
    "# Get input image size for first 5 images:\n",
    "IMAGES_PATH='flickr_dataset/flickr30k_images/flickr30k_images'\n",
    "for i in range(5):\n",
    "\n",
    "    sample_image_path = os.path.join(IMAGES_PATH, os.listdir(IMAGES_PATH)[i])\n",
    "    \n",
    "    # Open the image\n",
    "    image = Image.open(sample_image_path)\n",
    "    \n",
    "    # Get image size (width, height)\n",
    "    width, height = image.size\n",
    "    print(f\"Image dimensions: {width}x{height}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be1c92b-d7fe-4ed5-ade0-c3bb2ef04be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5254160a",
   "metadata": {},
   "source": [
    "\n",
    "#### The word distribution is peaked at ~15. I will choose a cutoff of 30 as majority of the images have caption length less than 30\n",
    "#### The image sizes are also variable, we will convert everything to 224x224 to make it standardized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d668b4d-86eb-4118-badd-270e506cc355",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = (224,224)  ### Varible image size in the input images. We have to resize to a fixed size\n",
    "SEQ_LENGTH = 30   ## remove the images with captions more than 30 words and add paddinf if it is less"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21eefe10-db0a-4c6d-a5ef-e4f1641eeb05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5aa95f6d",
   "metadata": {},
   "source": [
    "#### Prepare train, test and validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffbbf28-d4b3-4d0f-b822-ddbce3cd20ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_captions_data(filename):\n",
    "    with open(filename) as caption_file:\n",
    "        caption_data = caption_file.readlines()[1:]\n",
    "        caption_mapping = {}\n",
    "        text_data = []\n",
    "        images_to_skip = set()\n",
    "\n",
    "        for line in caption_data:\n",
    "            line = line.rstrip(\"\\n\")\n",
    "            try:\n",
    "                img_name, _, caption = line.split(\"| \")\n",
    "            except ValueError:\n",
    "                img_name, caption = line.split(\"| \")\n",
    "                caption = caption[4:]\n",
    "            img_name = os.path.join(IMAGES_PATH, img_name.strip())\n",
    "            tokens = caption.strip().split()\n",
    "            if len(tokens) < 4 or len(tokens) > SEQ_LENGTH:\n",
    "                images_to_skip.add(img_name)\n",
    "                continue\n",
    "            if img_name.endswith(\"jpg\") and img_name not in images_to_skip:\n",
    "                caption = \"<start> \" + caption.strip() + \" <end>\"\n",
    "                text_data.append(caption)\n",
    "\n",
    "                if img_name in caption_mapping:\n",
    "                    caption_mapping[img_name].append(caption)\n",
    "                else:\n",
    "                    caption_mapping[img_name] = [caption]\n",
    "\n",
    "        for img_name in images_to_skip:\n",
    "            if img_name in caption_mapping:\n",
    "                del caption_mapping[img_name]\n",
    "\n",
    "        return caption_mapping, text_data\n",
    "\n",
    "# Split dataset into train, validation, and test sets\n",
    "def train_val_split(caption_data, validation_size=0.2, test_size=0.02, shuffle=True):\n",
    "    all_images = list(caption_data.keys())\n",
    "    if shuffle:\n",
    "        np.random.seed(42)\n",
    "        np.random.shuffle(all_images)\n",
    "\n",
    "    train_keys, validation_keys = train_test_split(all_images, test_size=validation_size, random_state=42)\n",
    "    validation_keys, test_keys = train_test_split(validation_keys, test_size=test_size, random_state=42)\n",
    "    \n",
    "    training_data = {img_name: caption_data[img_name] for img_name in train_keys}\n",
    "    validation_data = {img_name: caption_data[img_name] for img_name in validation_keys}\n",
    "    test_data = {img_name: caption_data[img_name] for img_name in test_keys}\n",
    "\n",
    "    return training_data, validation_data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbb74ec",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f64a8b-293b-47ed-ba3d-ffa70f18b7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "captions_mapping, text_data = load_captions_data(CAPTIONS_PATH)\n",
    "train_data, validation_data, test_data = train_val_split(captions_mapping)\n",
    "print(f\"Total samples: {len(captions_mapping)}\")\n",
    "print(f\"Train samples: {len(train_data)}\")\n",
    "print(f\"Validation samples: {len(validation_data)}\")\n",
    "print(f\"Test samples: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361dae59-95fc-4b91-8682-148575cba9c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c00c8128",
   "metadata": {},
   "source": [
    "#### Visualize some the entries from the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a00a04c-932a-4a40-a8c8-94ad9ee5a6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualaization(data, num_of_images):\n",
    "    count = 1\n",
    "    fig = plt.figure(figsize=(10,20))\n",
    "    for filename in list(data.keys())[100:100+num_of_images]:\n",
    "        captions = data[filename]\n",
    "        image_load = Image.open(filename)\n",
    "\n",
    "        ax = fig.add_subplot(num_of_images,2,count,xticks=[],yticks=[])\n",
    "        ax.imshow(image_load)\n",
    "        count += 1\n",
    "\n",
    "        ax = fig.add_subplot(num_of_images,2,count)\n",
    "        plt.axis('off')\n",
    "        ax.plot()\n",
    "        ax.set_xlim(0,1)\n",
    "        ax.set_ylim(0,len(captions))\n",
    "        for i, caption in enumerate(captions):\n",
    "            #ax.text(0,i,caption,fontsize=20)\n",
    "            ax.text(0, i, caption.replace('<start>', '').replace('<end>', '').strip(), fontsize=20)\n",
    "        plt.savefig('Images_with_original_caption.pdf')\n",
    "        count += 1\n",
    "    plt.show()\n",
    "    \n",
    "visualaization(train_data, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7302a33",
   "metadata": {},
   "source": [
    "#### Tokenize the data and prepare it for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1824e52-c8d4-4832-bd8b-63261fc8197f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "\n",
    "class Flickr30kDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_dict, transform, tokenizer, max_length=30):\n",
    "        self.data_dict = data_dict\n",
    "        self.transform = transform\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.image_paths = list(data_dict.keys())\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        captions = self.data_dict[img_path]\n",
    "        \n",
    "        # Randomly select one caption for the image\n",
    "        caption = random.choice(captions)\n",
    "        \n",
    "        # Load and transform the image\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            image = self.transform(image)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_path}: {str(e)}\")\n",
    "            # Return a placeholder image\n",
    "            image = torch.zeros((3, 224, 224))\n",
    "        \n",
    "        # Tokenize the caption\n",
    "        tokens = self.tokenizer.encode(\n",
    "            caption,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        ).squeeze(0)\n",
    "        \n",
    "        return image, tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c58ba4-4cd6-485d-be9d-79eac0b61bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet stats\n",
    "])\n",
    "\n",
    "# Initialize BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = Flickr30kDataset(data_dict=train_data, transform=transform, tokenizer=tokenizer)\n",
    "val_dataset = Flickr30kDataset(data_dict=validation_data, transform=transform, tokenizer=tokenizer)\n",
    "test_dataset = Flickr30kDataset(data_dict=test_data, transform=transform, tokenizer=tokenizer)\n",
    "\n",
    "# Create DataLoader\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(train_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e00d89",
   "metadata": {},
   "source": [
    "#### Build a model for image caption generation. For the image part, we are useing pretrained resnet50 for encoding the image and for the text part, we are building a transformer decoder. We are using attention on individual space and texts are updated based on images as static memory. However, we are not using sophistifaced cross attention where both image and text emdedding updates one another"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a53e8fe-68fb-4f30-bf79-e2b9e2907e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(ImageEncoder, self).__init__()\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        modules = list(resnet.children())[:-2]  # Remove fully connected layer\n",
    "        self.cnn = nn.Sequential(*modules)\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((1, 1))  # Ensure output is (batch, 2048, 1, 1)\n",
    "        self.fc = nn.Linear(2048, embed_size)  # Reduce to embedding size\n",
    "        \n",
    "    def forward(self, images):\n",
    "        features = self.cnn(images)  # Output shape: (batch, 2048, 7, 7)\n",
    "        #print(\"CNN output shape:\", features.shape)\n",
    "        features = self.adaptive_pool(features)  # Now shape is (batch, 2048, 1, 1)\n",
    "        #print(\"CNN output shape after pool:\", features.shape)\n",
    "        features = features.view(features.size(0), -1)  # Flatten to (batch, 2048)\n",
    "        #print(\"CNN output shape flattening:\", features.shape)\n",
    "        features = self.fc(features)  # Output shape: (batch, embed_size)\n",
    "        return features\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d996952-f520-454f-b6b0-01d8a73afeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, embed_size, vocab_size, embedding_dim, nhead, num_layers):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        \n",
    "        # Embedding layer for captions\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.embed_size = embedding_dim\n",
    "        max_len = 100  # Maximum sequence length\n",
    "        pe = torch.zeros(max_len, embedding_dim)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, embedding_dim, 2).float() * -(torch.log(torch.tensor(10000.0)) / embedding_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))  # Shape: [1, max_len, embedding_dim]\n",
    "        \n",
    "        # Transformer decoder\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=embedding_dim, nhead=nhead)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Output linear layer\n",
    "        self.fc_out = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, image_features, captions):\n",
    "        # Embedding captions\n",
    "        embedded = self.embedding(captions)  # Shape: [batch_size, seq_len, embedding_dim]\n",
    "        \n",
    "        # Add positional encoding\n",
    "        seq_len = embedded.size(1)\n",
    "        positional_encoding = self.pe[:, :seq_len, :]  # Shape: [1, seq_len, embedding_dim]\n",
    "        embedded = embedded + positional_encoding\n",
    "        \n",
    "        # Prepare for transformer (requires [seq_len, batch_size, embedding_dim])\n",
    "        embedded = embedded.permute(1, 0, 2)\n",
    "        \n",
    "        # Prepare image features as memory for the transformer decoder\n",
    "        # Need to repeat the image features to act as memory for each position\n",
    "        # Shape of image_features: [batch_size, embed_size]\n",
    "        memory = image_features.unsqueeze(0).repeat(seq_len, 1, 1)  # Shape: [seq_len, batch_size, embed_size]\n",
    "        \n",
    "        # Create a mask to prevent attention to future tokens\n",
    "        mask = self.generate_square_subsequent_mask(seq_len).to(embedded.device)\n",
    "        \n",
    "        # Transformer decoder (output shape: [seq_len, batch_size, embedding_dim])\n",
    "        transformer_output = self.transformer_decoder(embedded, memory, tgt_mask=mask)\n",
    "        \n",
    "        # Permute back to [batch_size, seq_len, embedding_dim]\n",
    "        transformer_output = transformer_output.permute(1, 0, 2)\n",
    "        \n",
    "        # Project to vocabulary size\n",
    "        output = self.fc_out(transformer_output)  # Shape: [batch_size, seq_len, vocab_size]\n",
    "        \n",
    "        return output\n",
    "        \n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        # Generate a square mask for the sequence to prevent attending to future tokens\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "\n",
    "## Cross attention: more expensive. We are not using here\n",
    "\n",
    "class CrossAttentionTransformerDecoder(nn.Module):\n",
    "    def __init__(self, embed_size, vocab_size, embedding_dim, nhead, num_layers, max_text_len=100):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Text embedding\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # Image projection to match text embedding dimensions\n",
    "        self.image_projection = nn.Linear(embed_size, embedding_dim)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.positional_encoding = self.create_positional_encoding(embedding_dim, max_text_len + 1)  # +1 for image token\n",
    "        \n",
    "        # Transformer decoder layers with cross-attention\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=embedding_dim, \n",
    "            nhead=nhead,\n",
    "            dim_feedforward=embedding_dim * 4,  # Typically 4x embedding dim\n",
    "            dropout=0.1\n",
    "        )\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers)\n",
    "        \n",
    "        # Output layer\n",
    "        self.fc_out = nn.Linear(embedding_dim, vocab_size)\n",
    "    \n",
    "    def create_positional_encoding(self, embed_size, max_len=101):\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_size, 2).float() * -(torch.log(torch.tensor(10000.0)) / embed_size))\n",
    "        pe = torch.zeros(max_len, embed_size)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pe.unsqueeze(0)\n",
    "    \n",
    "    def forward(self, image_features, captions):\n",
    "        # Project image features to text embedding dimension\n",
    "        projected_image = self.image_projection(image_features)  # [batch_size, embedding_dim]\n",
    "        \n",
    "        # Embed captions\n",
    "        text_embeds = self.embedding(captions)  # [batch_size, seq_len, embedding_dim]\n",
    "        \n",
    "        # Concatenate image token with text tokens\n",
    "        # Add a special image token at the beginning\n",
    "        image_token = projected_image.unsqueeze(1)  # [batch_size, 1, embedding_dim]\n",
    "        combined_embeds = torch.cat([image_token, text_embeds], dim=1)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        seq_len = combined_embeds.size(1)\n",
    "        pos_encoding = self.positional_encoding[:, :seq_len, :].to(combined_embeds.device)\n",
    "        combined_embeds = combined_embeds + pos_encoding\n",
    "        \n",
    "        # Prepare for transformer (requires [seq_len, batch_size, embedding_dim])\n",
    "        combined_embeds = combined_embeds.permute(1, 0, 2)\n",
    "        \n",
    "        # Create mask to prevent attending to future tokens (now including the image token)\n",
    "        mask = self.generate_square_subsequent_mask(seq_len).to(combined_embeds.device)\n",
    "        \n",
    "        # Transformer decoder\n",
    "        # Use combined_embeds as both target and memory to enable cross-attention\n",
    "        transformer_output = self.transformer_decoder(\n",
    "            tgt=combined_embeds,  # Target sequence (image + text)\n",
    "            memory=combined_embeds,  # Memory (same as target for cross-attention)\n",
    "            tgt_mask=mask  # Mask to prevent attending to future tokens\n",
    "        )\n",
    "        \n",
    "        # Permute back and remove image token\n",
    "        transformer_output = transformer_output.permute(1, 0, 2)[:, 1:, :]\n",
    "        \n",
    "        # Project to vocabulary size\n",
    "        output = self.fc_out(transformer_output)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        # Generate a square mask for the sequence to prevent attending to future tokens\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16bb0e2-2fc1-49c4-a927-e87b35cb6c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we want to use the cross attention, this class need to be modified. \n",
    "class ImageCaptioningModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, embedding_dim, nhead, num_layers):\n",
    "        super(ImageCaptioningModel, self).__init__()\n",
    "        self.image_encoder = ImageEncoder(embed_size)\n",
    "        self.transformer_decoder = TransformerDecoder(\n",
    "        #self.transformer_decoder = CrossAttentionTransformerDecoder(\n",
    "            embed_size=embed_size,\n",
    "            vocab_size=vocab_size,\n",
    "            embedding_dim=embedding_dim,\n",
    "            nhead=nhead,\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        \n",
    "    def forward(self, images, captions):\n",
    "        image_features = self.image_encoder(images)\n",
    "        output = self.transformer_decoder(image_features, captions)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e92432a-6ec5-4d49-bfa0-a1213f6491d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, criterion, optimizer, num_epochs, device, vocab_size):\n",
    "    # Lists to store losses for plotting\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        \n",
    "        for images, captions in tqdm(iter(train_loader), total=len(train_loader)):\n",
    "            images = images.to(device)\n",
    "            captions = captions.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass (exclude last token from input)\n",
    "            outputs = model(images, captions[:, :-1])\n",
    "            \n",
    "            # Reshape outputs and captions for loss calculation\n",
    "            outputs = outputs.contiguous().view(-1, vocab_size)\n",
    "            targets = captions[:, 1:].contiguous().view(-1)  # Skip the first token (usually BOS)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Clip gradients to avoid exploding gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            total_train_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        \n",
    "        # Validate the model\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for images, captions in tqdm(iter(val_loader), total=len(val_loader)):\n",
    "                images = images.to(device)\n",
    "                captions = captions.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(images, captions[:, :-1])\n",
    "                \n",
    "                # Reshape outputs and captions for loss calculation\n",
    "                outputs = outputs.contiguous().view(-1, vocab_size)\n",
    "                targets = captions[:, 1:].contiguous().view(-1)\n",
    "                \n",
    "                # Calculate loss\n",
    "                loss = criterion(outputs, targets)\n",
    "                total_val_loss += loss.item()\n",
    "        \n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        \n",
    "        # Save the losses for plotting\n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "        \n",
    "        # Save checkpoint\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': avg_train_loss,\n",
    "                'val_loss': avg_val_loss,\n",
    "            }, f\"checkpoint_epoch_{epoch+1}.pth\")\n",
    "    \n",
    "    return train_losses, val_losses\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4780fd-da40-4c54-bf44-0cf1cf86fb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model parameters\n",
    "embed_size = 512        # Embedding size for image features\n",
    "embedding_dim = 512     # Embedding dimension for captions\n",
    "nhead = 8               # Number of attention heads\n",
    "num_layers = 6          # Number of transformer layers\n",
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "# Create the image captioning model\n",
    "model = ImageCaptioningModel(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_size=embed_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    nhead=nhead,\n",
    "    num_layers=num_layers\n",
    ")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e997fe-6e81-47a2-a084-0f5b031417dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move model to device (GPU if available, otherwise CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Now you can start training\n",
    "train_losses, val_losses = train(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    num_epochs=10,\n",
    "    device=device,\n",
    "    vocab_size=vocab_size\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ed0006-f029-4e73-9ccb-5b0053db3894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training, you can plot the losses\n",
    "import matplotlib.pyplot as plt\n",
    "num_epochs=10\n",
    "plt.plot(range(1, num_epochs+1), train_losses, label='Training Loss')\n",
    "plt.plot(range(1, num_epochs+1), val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.savefig('training_curve.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f061190",
   "metadata": {},
   "source": [
    "#### So, training for 10 epochs are done. Training loss still seems to reduce significantly, however, validation loss does not decrease much. It could be a sign of overfitting. Let's see how the train model perform using different metrices such as BLEUs and METEOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63132635-04aa-483e-a2b5-765801777324",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1002c4f6-ce7f-47a3-b627-ed25698ced29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f68ac09-60ba-4fc8-a812-03bae0ba5a90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4d57e8-b7b9-42e1-b82f-c68d137cfceb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5f290c-b8bb-4946-bc21-7b72e6af6cf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a1a81c-28aa-485a-b094-08259f09b018",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83333b50-e48a-45ee-9f1e-a3fbfe68db94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3739c6e3-5a59-41f0-beac-19eed45b7f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from nltk.translate.bleu_score import corpus_bleu, sentence_bleu\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "def load_model(model_path, model_class, device):\n",
    "    \"\"\"\n",
    "    Load a trained model from checkpoint\n",
    "    \"\"\"\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    model = model_class(\n",
    "        vocab_size=tokenizer.vocab_size,\n",
    "        embed_size=512,\n",
    "        embedding_dim=512,\n",
    "        nhead=8,\n",
    "        num_layers=6\n",
    "    )\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def generate_caption(model, image_path, tokenizer, transform, max_length=30, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Generate a caption for a given image\n",
    "    \"\"\"\n",
    "    # Load and preprocess the image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = transform(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Initialize caption generation with start token\n",
    "    start_token_id = tokenizer.convert_tokens_to_ids(['[CLS]'])[0]\n",
    "    caption = torch.tensor([[start_token_id]]).to(device)\n",
    "    \n",
    "    # Image encoding\n",
    "    with torch.no_grad():\n",
    "        image_features = model.image_encoder(image)\n",
    "    \n",
    "    # Generate caption word by word\n",
    "    for _ in range(max_length):\n",
    "        with torch.no_grad():\n",
    "            output = model(image, caption)\n",
    "            # Get the next word prediction (last token)\n",
    "            predicted = output[:, -1, :].argmax(dim=1, keepdim=True)\n",
    "            \n",
    "            # Append the predicted word to the caption\n",
    "            caption = torch.cat([caption, predicted], dim=1)\n",
    "            \n",
    "            # Stop if end token is predicted\n",
    "            if predicted.item() == tokenizer.sep_token_id:\n",
    "                break\n",
    "    \n",
    "    # Convert caption from token ids to text\n",
    "    caption_text = tokenizer.decode(caption[0].tolist(), skip_special_tokens=True)\n",
    "    return caption_text\n",
    "\n",
    "def evaluate_model(model, test_data, tokenizer, transform, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Evaluate the model on test data and calculate BLEU scores\n",
    "    \"\"\"\n",
    "    references = []\n",
    "    hypotheses = []\n",
    "    \n",
    "    for img_path, captions in tqdm(test_data.items()):\n",
    "        if not os.path.exists(img_path):\n",
    "            continue\n",
    "            \n",
    "        # Generate caption\n",
    "        generated_caption = generate_caption(model, img_path, tokenizer, transform, device=device)\n",
    "        \n",
    "        # Process reference captions\n",
    "        reference_captions = []\n",
    "        for caption in captions:\n",
    "            # Clean up reference caption - remove <start> and <end>\n",
    "            clean_caption = caption.replace(\"<start>\", \"\").replace(\"<end>\", \"\").strip()\n",
    "            reference_captions.append(clean_caption.split())\n",
    "        \n",
    "        # Process generated caption\n",
    "        generated_caption = generated_caption.split()\n",
    "        \n",
    "        # Add to lists for corpus calculation\n",
    "        references.append(reference_captions)\n",
    "        hypotheses.append(generated_caption)\n",
    "    \n",
    "    # Calculate BLEU scores\n",
    "    bleu1 = corpus_bleu(references, hypotheses, weights=(1, 0, 0, 0))\n",
    "    bleu2 = corpus_bleu(references, hypotheses, weights=(0.5, 0.5, 0, 0))\n",
    "    bleu3 = corpus_bleu(references, hypotheses, weights=(0.33, 0.33, 0.33, 0))\n",
    "    bleu4 = corpus_bleu(references, hypotheses, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "    \n",
    "    # Calculate METEOR score\n",
    "    meteor_scores = []\n",
    "    for i in range(len(references)):\n",
    "        score = meteor_score(references[i], hypotheses[i])\n",
    "        meteor_scores.append(score)\n",
    "    meteor_avg = np.mean(meteor_scores)\n",
    "    \n",
    "    return {\n",
    "        'BLEU-1': bleu1 * 100,\n",
    "        'BLEU-2': bleu2 * 100,\n",
    "        'BLEU-3': bleu3 * 100,\n",
    "        'BLEU-4': bleu4 * 100,\n",
    "        'METEOR': meteor_avg * 100\n",
    "    }\n",
    "\n",
    "def visualize_examples(model, test_data, tokenizer, transform, num_examples=5, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Visualize some example predictions\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import random\n",
    "    \n",
    "    # Select random images from test data\n",
    "    img_paths = list(test_data.keys())\n",
    "    selected_paths = random.sample(img_paths, min(num_examples, len(img_paths)))\n",
    "    \n",
    "    fig, axes = plt.subplots(num_examples, 1, figsize=(15, 5*num_examples))\n",
    "    \n",
    "    for i, img_path in enumerate(selected_paths):\n",
    "        if not os.path.exists(img_path):\n",
    "            continue\n",
    "            \n",
    "        # Generate caption\n",
    "        generated_caption = generate_caption(model, img_path, tokenizer, transform, device=device)\n",
    "        \n",
    "        # Reference captions\n",
    "        reference_captions = [caption.replace(\"<start>\", \"\").replace(\"<end>\", \"\").strip() \n",
    "                             for caption in test_data[img_path]]\n",
    "        \n",
    "        # Display image and captions\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        if num_examples > 1:\n",
    "            ax = axes[i]\n",
    "        else:\n",
    "            ax = axes\n",
    "            \n",
    "        ax.imshow(img)\n",
    "        ax.set_title(f\"Generated: {generated_caption}\")\n",
    "        caption_text = \"\\n\".join([f\"Reference {j+1}: {ref}\" for j, ref in enumerate(reference_captions)])\n",
    "        ax.set_xlabel(caption_text)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"caption_examples.png\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def prepare_nltk_data():\n",
    "    \"\"\"\n",
    "    Download and verify required NLTK data packages\n",
    "    \"\"\"\n",
    "    import nltk\n",
    "    import ssl\n",
    "    \n",
    "    # Try to work around SSL certificate issues that sometimes occur\n",
    "    try:\n",
    "        _create_unverified_https_context = ssl._create_unverified_context\n",
    "    except AttributeError:\n",
    "        pass\n",
    "    else:\n",
    "        ssl._create_default_https_context = _create_unverified_https_context\n",
    "    \n",
    "    # Download required packages\n",
    "    nltk.download('punkt', quiet=False)\n",
    "    nltk.download('wordnet', quiet=False)\n",
    "    nltk.download('omw-1.4', quiet=False)\n",
    "    \n",
    "    # Verify packages are properly installed\n",
    "    try:\n",
    "        from nltk.corpus import wordnet\n",
    "        synsets = wordnet.synsets('test')\n",
    "        if not synsets:\n",
    "            print(\"WARNING: WordNet seems installed but returned no synsets - this may indicate a problem\")\n",
    "    except LookupError:\n",
    "        print(\"ERROR: WordNet isn't properly installed despite download attempt\")\n",
    "        print(\"If you're running in a restricted environment, you may need to:\")\n",
    "        print(\"1. Download the data manually on another machine\")\n",
    "        print(\"2. Copy the 'nltk_data' directory to one of the search paths listed in the error\")\n",
    "\n",
    "# Alternative evaluation function without METEOR to avoid WordNet dependency\n",
    "def evaluate_model_bleu_only(model, test_data, tokenizer, transform, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Evaluate the model on test data using only BLEU scores (no METEOR)\n",
    "    \"\"\"\n",
    "    references = []\n",
    "    hypotheses = []\n",
    "    \n",
    "    for img_path, captions in tqdm(test_data.items()):\n",
    "        if not os.path.exists(img_path):\n",
    "            continue\n",
    "            \n",
    "        # Generate caption\n",
    "        generated_caption = generate_caption(model, img_path, tokenizer, transform, device=device)\n",
    "        \n",
    "        # Process reference captions\n",
    "        reference_captions = []\n",
    "        for caption in captions:\n",
    "            # Clean up reference caption - remove <start> and <end>\n",
    "            clean_caption = caption.replace(\"<start>\", \"\").replace(\"<end>\", \"\").strip()\n",
    "            reference_captions.append(clean_caption.split())\n",
    "        \n",
    "        # Process generated caption\n",
    "        generated_caption = generated_caption.split()\n",
    "        \n",
    "        # Add to lists for corpus calculation\n",
    "        references.append(reference_captions)\n",
    "        hypotheses.append(generated_caption)\n",
    "    \n",
    "    # Calculate BLEU scores\n",
    "    bleu1 = corpus_bleu(references, hypotheses, weights=(1, 0, 0, 0))\n",
    "    bleu2 = corpus_bleu(references, hypotheses, weights=(0.5, 0.5, 0, 0))\n",
    "    bleu3 = corpus_bleu(references, hypotheses, weights=(0.33, 0.33, 0.33, 0))\n",
    "    bleu4 = corpus_bleu(references, hypotheses, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "    \n",
    "    return {\n",
    "        'BLEU-1': bleu1 * 100,\n",
    "        'BLEU-2': bleu2 * 100,\n",
    "        'BLEU-3': bleu3 * 100,\n",
    "        'BLEU-4': bleu4 * 100\n",
    "    }\n",
    "\n",
    "# Update in your main function:\n",
    "def main():\n",
    "    # Prepare NLTK data\n",
    "    prepare_nltk_data()\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Load model\n",
    "    checkpoint_path = \"checkpoint_epoch_10.pth\"  # Update with your checkpoint path\n",
    "    model = load_model(checkpoint_path, ImageCaptioningModel, device)\n",
    "    \n",
    "    try:\n",
    "        # Try to evaluate with BLEU and METEOR\n",
    "        metrics = evaluate_model(model, test_data, tokenizer, transform, device=device)\n",
    "    except LookupError as e:\n",
    "        print(f\"Error with full evaluation: {e}\")\n",
    "        print(\"Falling back to BLEU-only evaluation\")\n",
    "        # Fall back to BLEU-only evaluation\n",
    "        metrics = evaluate_model_bleu_only(model, test_data, tokenizer, transform, device=device)\n",
    "    \n",
    "    print(\"Evaluation Metrics:\")\n",
    "    for metric_name, score in metrics.items():\n",
    "        print(f\"{metric_name}: {score:.2f}%\")\n",
    "    \n",
    "    # Visualize some examples\n",
    "    visualize_examples(model, test_data, tokenizer, transform, num_examples=5, device=device)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9ee94c-9d50-4898-91ba-3be0cf4a4d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Let's see how it performs on a image outside this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c428e73b-9b29-422a-b650-3636373ca0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def caption_single_image(image_path, model, tokenizer, transform, max_length=30, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Generate caption for a single custom image\n",
    "    \"\"\"\n",
    "    # Load and preprocess the image\n",
    "    try:\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "        \n",
    "        # Set model to evaluation mode\n",
    "        model.eval()\n",
    "        \n",
    "        # Start with BERT's [CLS] token (equivalent to <start> in your training)\n",
    "        start_token = tokenizer.cls_token_id\n",
    "        caption = torch.tensor([[start_token]]).to(device)\n",
    "        \n",
    "        # Generate caption token by token\n",
    "        with torch.no_grad():\n",
    "            # Get image features\n",
    "            image_features = model.image_encoder(image_tensor)\n",
    "            \n",
    "            # Generate tokens one by one\n",
    "            for _ in range(max_length):\n",
    "                # Get model prediction\n",
    "                output = model.transformer_decoder(image_features, caption)\n",
    "                \n",
    "                # Get the next word prediction (last token)\n",
    "                predicted = output[:, -1, :].argmax(dim=1, keepdim=True)\n",
    "                \n",
    "                # Append the predicted token to the caption\n",
    "                caption = torch.cat([caption, predicted], dim=1)\n",
    "                \n",
    "                # Stop if end token is predicted (BERT's [SEP] token)\n",
    "                if predicted.item() == tokenizer.sep_token_id:\n",
    "                    break\n",
    "        \n",
    "        # Convert tokens to text\n",
    "        caption_text = tokenizer.decode(caption[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Display the image and caption\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(image)\n",
    "        plt.title(f\"Caption: {caption_text}\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        \n",
    "        return caption_text\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Usage example:\n",
    "# First load your model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "checkpoint_path = \"checkpoint_epoch_10.pth\"\n",
    "model = load_model(checkpoint_path, ImageCaptioningModel, device)\n",
    "#\n",
    "# Then generate caption for your image\n",
    "test_image_path = \"generate_test.JPG\"\n",
    "caption = caption_single_image(test_image_path, model, tokenizer, transform, device=device)\n",
    "print(f\"Generated caption: {caption}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283ccf33-cb36-4ebb-b822-e643fe0df749",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bae911-0f7e-438a-a95f-fb0903859c79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f79182-0631-41f7-8343-bb633509a914",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fb529e-9e40-4025-b977-96f84074d547",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdea736-5d25-44dc-90cd-9ec2e004aec1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "97f90292",
   "metadata": {},
   "source": [
    "#### Okay, the BLEU score and METEOR score above >30% is very good considering how small of a dataset we are using. However, there are still room for improvements. Let's try the cross attention model and see if we can improve "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da46e27d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8349f81-677c-478f-829b-9f8c66bf67d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c696dada-b965-4319-aa2c-17aaf6c593af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3e1081-83fd-40f6-becd-bc467b1b52db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35431eba-1a9c-453f-be62-5df6b5280bf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881b96fc-e4eb-40f7-81af-18949ab256f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47d99d5-d1cc-44ca-9591-b76cde3f37aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2886020c-c8f3-49d7-8fe5-6f9aae3d7e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(ImageEncoder, self).__init__()\n",
    "        # Use a pre-trained ResNet model\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        # Remove the last fully connected layer\n",
    "        modules = list(resnet.children())[:-1]\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        \n",
    "        # Add a projection layer to get the desired embedding size\n",
    "        self.linear = nn.Linear(resnet.fc.in_features, embed_size)\n",
    "        self.bn = nn.BatchNorm1d(embed_size)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "        # Freeze the ResNet parameters to speed up training\n",
    "        for param in self.resnet.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "    def forward(self, images):\n",
    "        # Extract features from the image\n",
    "        with torch.no_grad():\n",
    "            features = self.resnet(images)\n",
    "        \n",
    "        # Reshape features to [batch_size, resnet.fc.in_features]\n",
    "        features = features.view(features.size(0), -1)\n",
    "        \n",
    "        # Project features to embed_size dimension\n",
    "        features = self.linear(features)\n",
    "        features = self.bn(features)\n",
    "        features = self.dropout(features)\n",
    "        \n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e83f18-a869-457f-afaa-48a0fc056f64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e190d946-7794-428d-86ed-3c71219af513",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5118d0c0-888d-48f9-bd36-4f270a2810bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttentionTransformerDecoder(nn.Module):\n",
    "    def __init__(self, embed_size, vocab_size, embedding_dim, nhead, num_layers):\n",
    "        super(CrossAttentionTransformerDecoder, self).__init__()\n",
    "        \n",
    "        # Embedding layer for captions\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.embed_size = embedding_dim\n",
    "        max_len = 100  # Maximum sequence length\n",
    "        pe = torch.zeros(max_len, embedding_dim)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, embedding_dim, 2).float() * -(torch.log(torch.tensor(10000.0)) / embedding_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))  # Shape: [1, max_len, embedding_dim]\n",
    "        \n",
    "        # Linear layer to project image features to match decoder dimensions\n",
    "        self.img_projection = nn.Linear(embed_size, embedding_dim)\n",
    "        \n",
    "        # Custom transformer decoder layers with cross-attention\n",
    "        self.decoder_layers = nn.ModuleList([\n",
    "            DecoderLayerWithCrossAttention(embedding_dim, nhead)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Output linear layer\n",
    "        self.fc_out = nn.Linear(embedding_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, image_features, captions):\n",
    "        # Embedding captions\n",
    "        embedded = self.embedding(captions)  # Shape: [batch_size, seq_len, embedding_dim]\n",
    "        \n",
    "        # Add positional encoding\n",
    "        seq_len = embedded.size(1)\n",
    "        positional_encoding = self.pe[:, :seq_len, :]  # Shape: [1, seq_len, embedding_dim]\n",
    "        embedded = embedded + positional_encoding\n",
    "        \n",
    "        # Project image features to match embedding dimension\n",
    "        image_features = self.img_projection(image_features)  # Shape: [batch_size, embedding_dim]\n",
    "        \n",
    "        # Create self-attention mask to prevent attention to future tokens\n",
    "        tgt_mask = self.generate_square_subsequent_mask(seq_len).to(embedded.device)\n",
    "        \n",
    "        # Process through decoder layers\n",
    "        output = embedded\n",
    "        for decoder_layer in self.decoder_layers:\n",
    "            output = decoder_layer(output, image_features, tgt_mask)\n",
    "        \n",
    "        # Project to vocabulary size\n",
    "        output = self.fc_out(output)  # Shape: [batch_size, seq_len, vocab_size]\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        # Generate a square mask for the sequence to prevent attending to future tokens\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7846c3f-d2c4-4432-baab-d27bbf966607",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayerWithCrossAttention(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dropout=0.1):\n",
    "        super(DecoderLayerWithCrossAttention, self).__init__()\n",
    "        \n",
    "        # Self-attention\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        \n",
    "        # Cross-attention between text and image features\n",
    "        self.cross_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        \n",
    "        # Feed-forward network\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, 4 * d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(4 * d_model, d_model)\n",
    "        )\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, tgt, memory, tgt_mask=None):\n",
    "        # Self-attention\n",
    "        tgt2 = self.self_attn(\n",
    "            query=tgt.transpose(0, 1),\n",
    "            key=tgt.transpose(0, 1),\n",
    "            value=tgt.transpose(0, 1),\n",
    "            attn_mask=tgt_mask\n",
    "        )[0].transpose(0, 1)\n",
    "        tgt = self.norm1(tgt + self.dropout1(tgt2))\n",
    "        \n",
    "        # Cross-attention between text and image\n",
    "        # Expand image features to be compatible with text sequence\n",
    "        batch_size, seq_len, d_model = tgt.size()\n",
    "        expanded_memory = memory.unsqueeze(1).expand(-1, seq_len, -1)  # [batch_size, seq_len, d_model]\n",
    "        \n",
    "        # Prepare for cross-attention\n",
    "        q = tgt.transpose(0, 1)  # [seq_len, batch_size, d_model]\n",
    "        k = expanded_memory.transpose(0, 1)  # [seq_len, batch_size, d_model]\n",
    "        v = expanded_memory.transpose(0, 1)  # [seq_len, batch_size, d_model]\n",
    "        \n",
    "        # Apply cross-attention\n",
    "        tgt2 = self.cross_attn(q, k, v)[0].transpose(0, 1)\n",
    "        tgt = self.norm2(tgt + self.dropout2(tgt2))\n",
    "        \n",
    "        # Feed-forward network\n",
    "        tgt2 = self.ffn(tgt)\n",
    "        tgt = self.norm3(tgt + self.dropout3(tgt2))\n",
    "        \n",
    "        return tgt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c693d3-4d86-4ac8-8893-01a01f1bb703",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptioningModelWithCrossAttention(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, embedding_dim, nhead, num_layers):\n",
    "        super(ImageCaptioningModelWithCrossAttention, self).__init__()\n",
    "        \n",
    "        self.image_encoder = ImageEncoder(embed_size)\n",
    "        self.transformer_decoder = CrossAttentionTransformerDecoder(\n",
    "            embed_size=embed_size,\n",
    "            vocab_size=vocab_size,\n",
    "            embedding_dim=embedding_dim,\n",
    "            nhead=nhead,\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        \n",
    "    def forward(self, images, captions):\n",
    "        image_features = self.image_encoder(images)\n",
    "        output = self.transformer_decoder(image_features, captions)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f52c0c-3d40-4429-bbb7-ed852b65ab68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, criterion, optimizer, num_epochs, device, vocab_size):\n",
    "    # Lists to store losses for plotting\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        \n",
    "        for images, captions in tqdm(iter(train_loader), total=len(train_loader)):\n",
    "            images = images.to(device)\n",
    "            captions = captions.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass (exclude last token from input)\n",
    "            outputs = model(images, captions[:, :-1])\n",
    "            \n",
    "            # Reshape outputs and captions for loss calculation\n",
    "            outputs = outputs.contiguous().view(-1, vocab_size)\n",
    "            targets = captions[:, 1:].contiguous().view(-1)  # Skip the first token (usually BOS)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Clip gradients to avoid exploding gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            total_train_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        \n",
    "        # Validate the model\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for images, captions in tqdm(iter(val_loader), total=len(val_loader)):\n",
    "                images = images.to(device)\n",
    "                captions = captions.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(images, captions[:, :-1])\n",
    "                \n",
    "                # Reshape outputs and captions for loss calculation\n",
    "                outputs = outputs.contiguous().view(-1, vocab_size)\n",
    "                targets = captions[:, 1:].contiguous().view(-1)\n",
    "                \n",
    "                # Calculate loss\n",
    "                loss = criterion(outputs, targets)\n",
    "                total_val_loss += loss.item()\n",
    "        \n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        \n",
    "        # Save the losses for plotting\n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "        \n",
    "        # Save checkpoint\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': avg_train_loss,\n",
    "                'val_loss': avg_val_loss,\n",
    "            }, f\"CA2_checkpoint_epoch_{epoch+1}.pth\")\n",
    "    \n",
    "    return train_losses, val_losses\n",
    "\n",
    "# Initialize model parameters\n",
    "embed_size = 512        # Embedding size for image features\n",
    "embedding_dim = 512     # Embedding dimension for captions\n",
    "nhead = 8               # Number of attention heads\n",
    "num_layers = 6          # Number of transformer layers\n",
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "# Create the image captioning model\n",
    "model = ImageCaptioningModelWithCrossAttention(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_size=embed_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    nhead=nhead,\n",
    "    num_layers=num_layers\n",
    ")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ce6497-e794-4075-9a9f-0f52bca77f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "# Move model to device (GPU if available, otherwise CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Define loss function and optimizer - this stays exactly the same\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Now you can start training with your existing train function\n",
    "train_losses, val_losses = train(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    num_epochs=30,\n",
    "    device=device,\n",
    "    vocab_size=vocab_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad6538e-e315-4b80-a029-34b635dc269a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training, you can plot the losses\n",
    "import matplotlib.pyplot as plt\n",
    "num_epochs=30\n",
    "plt.plot(range(1, num_epochs+1), train_losses, label='Training Loss')\n",
    "plt.plot(range(1, num_epochs+1), val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.savefig('training_curve_cross_attention.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41acbd3",
   "metadata": {},
   "source": [
    "#### Interesting. This is a classic example of overfitting. Training loss reducing significantly, but validation loss is constant. Let's check the perfomance first. If we compare carefully, upto 10 epochs, the losses are pretty much similar as our previous model without cross attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64471741-17b6-467e-92d7-63b32bd0d5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu, sentence_bleu\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "import os\n",
    "\n",
    "def load_model(model_path, model_class, device):\n",
    "    \"\"\"\n",
    "    Load a trained cross-attention model from checkpoint\n",
    "    \"\"\"\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    model = model_class(\n",
    "        vocab_size=tokenizer.vocab_size,\n",
    "        embed_size=512,  # Ensure these match your training configuration\n",
    "        embedding_dim=512,\n",
    "        nhead=8,\n",
    "        num_layers=6\n",
    "    )\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def generate_caption(model, image_path, tokenizer, transform, max_length=30, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Generate a caption for a given image using cross-attention model\n",
    "    \"\"\"\n",
    "    # Load and preprocess the image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = transform(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Initialize caption generation with start token\n",
    "    start_token_id = tokenizer.convert_tokens_to_ids(['[CLS]'])[0]\n",
    "    caption = torch.tensor([[start_token_id]]).to(device)\n",
    "    \n",
    "    # Image encoding (get features before projection)\n",
    "    with torch.no_grad():\n",
    "        image_features = model.image_encoder(image)\n",
    "    \n",
    "    # Generate caption word by word\n",
    "    for _ in range(max_length):\n",
    "        with torch.no_grad():\n",
    "            # Key change: Pass image features and current caption\n",
    "            output = model.transformer_decoder(image_features, caption)\n",
    "            \n",
    "            # Get the next word prediction (last token)\n",
    "            predicted = output[:, -1, :].argmax(dim=1, keepdim=True)\n",
    "            \n",
    "            # Append the predicted word to the caption\n",
    "            caption = torch.cat([caption, predicted], dim=1)\n",
    "            \n",
    "            # Stop if end token is predicted\n",
    "            if predicted.item() == tokenizer.sep_token_id:\n",
    "                break\n",
    "    \n",
    "    # Convert caption from token ids to text\n",
    "    caption_text = tokenizer.decode(caption[0].tolist(), skip_special_tokens=True)\n",
    "    return caption_text\n",
    "\n",
    "\n",
    "def evaluate_model(model, test_data, tokenizer, transform, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Evaluate the model on test data and calculate BLEU scores\n",
    "    \"\"\"\n",
    "    references = []\n",
    "    hypotheses = []\n",
    "    \n",
    "    for img_path, captions in tqdm(test_data.items()):\n",
    "        if not os.path.exists(img_path):\n",
    "            continue\n",
    "            \n",
    "        # Generate caption\n",
    "        generated_caption = generate_caption(model, img_path, tokenizer, transform, device=device)\n",
    "        \n",
    "        # Process reference captions\n",
    "        reference_captions = []\n",
    "        for caption in captions:\n",
    "            # Clean up reference caption - remove <start> and <end>\n",
    "            clean_caption = caption.replace(\"<start>\", \"\").replace(\"<end>\", \"\").strip()\n",
    "            reference_captions.append(clean_caption.split())\n",
    "        \n",
    "        # Process generated caption\n",
    "        generated_caption = generated_caption.split()\n",
    "        \n",
    "        # Add to lists for corpus calculation\n",
    "        references.append(reference_captions)\n",
    "        hypotheses.append(generated_caption)\n",
    "    \n",
    "    # Calculate BLEU scores\n",
    "    bleu1 = corpus_bleu(references, hypotheses, weights=(1, 0, 0, 0))\n",
    "    bleu2 = corpus_bleu(references, hypotheses, weights=(0.5, 0.5, 0, 0))\n",
    "    bleu3 = corpus_bleu(references, hypotheses, weights=(0.33, 0.33, 0.33, 0))\n",
    "    bleu4 = corpus_bleu(references, hypotheses, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "    \n",
    "    # Calculate METEOR score\n",
    "    meteor_scores = []\n",
    "    for i in range(len(references)):\n",
    "        score = meteor_score(references[i], hypotheses[i])\n",
    "        meteor_scores.append(score)\n",
    "    meteor_avg = np.mean(meteor_scores)\n",
    "    \n",
    "    return {\n",
    "        'BLEU-1': bleu1 * 100,\n",
    "        'BLEU-2': bleu2 * 100,\n",
    "        'BLEU-3': bleu3 * 100,\n",
    "        'BLEU-4': bleu4 * 100,\n",
    "        'METEOR': meteor_avg * 100\n",
    "    }\n",
    "\n",
    "def visualize_examples(model, test_data, tokenizer, transform, num_examples=5, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Visualize some example predictions\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import random\n",
    "    \n",
    "    # Select random images from test data\n",
    "    img_paths = list(test_data.keys())\n",
    "    selected_paths = random.sample(img_paths, min(num_examples, len(img_paths)))\n",
    "    \n",
    "    fig, axes = plt.subplots(num_examples, 1, figsize=(15, 5*num_examples))\n",
    "    \n",
    "    for i, img_path in enumerate(selected_paths):\n",
    "        if not os.path.exists(img_path):\n",
    "            continue\n",
    "            \n",
    "        # Generate caption\n",
    "        generated_caption = generate_caption(model, img_path, tokenizer, transform, device=device)\n",
    "        \n",
    "        # Reference captions\n",
    "        reference_captions = [caption.replace(\"<start>\", \"\").replace(\"<end>\", \"\").strip() \n",
    "                             for caption in test_data[img_path]]\n",
    "        \n",
    "        # Display image and captions\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        if num_examples > 1:\n",
    "            ax = axes[i]\n",
    "        else:\n",
    "            ax = axes\n",
    "            \n",
    "        ax.imshow(img)\n",
    "        ax.set_title(f\"Generated: {generated_caption}\")\n",
    "        caption_text = \"\\n\".join([f\"Reference {j+1}: {ref}\" for j, ref in enumerate(reference_captions)])\n",
    "        ax.set_xlabel(caption_text)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"caption_examples_cross_attention.png\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def prepare_nltk_data():\n",
    "    \"\"\"\n",
    "    Download and verify required NLTK data packages\n",
    "    \"\"\"\n",
    "    import nltk\n",
    "    import ssl\n",
    "    \n",
    "    # Try to work around SSL certificate issues that sometimes occur\n",
    "    try:\n",
    "        _create_unverified_https_context = ssl._create_unverified_context\n",
    "    except AttributeError:\n",
    "        pass\n",
    "    else:\n",
    "        ssl._create_default_https_context = _create_unverified_https_context\n",
    "    \n",
    "    # Download required packages\n",
    "    nltk.download('punkt', quiet=False)\n",
    "    nltk.download('wordnet', quiet=False)\n",
    "    nltk.download('omw-1.4', quiet=False)\n",
    "    \n",
    "    # Verify packages are properly installed\n",
    "    try:\n",
    "        from nltk.corpus import wordnet\n",
    "        synsets = wordnet.synsets('test')\n",
    "        if not synsets:\n",
    "            print(\"WARNING: WordNet seems installed but returned no synsets - this may indicate a problem\")\n",
    "    except LookupError:\n",
    "        print(\"ERROR: WordNet isn't properly installed despite download attempt\")\n",
    "        print(\"If you're running in a restricted environment, you may need to:\")\n",
    "        print(\"1. Download the data manually on another machine\")\n",
    "        print(\"2. Copy the 'nltk_data' directory to one of the search paths listed in the error\")\n",
    "\n",
    "# Alternative evaluation function without METEOR to avoid WordNet dependency\n",
    "def evaluate_model_bleu_only(model, test_data, tokenizer, transform, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Evaluate the model on test data using only BLEU scores (no METEOR)\n",
    "    \"\"\"\n",
    "    references = []\n",
    "    hypotheses = []\n",
    "    \n",
    "    for img_path, captions in tqdm(test_data.items()):\n",
    "        if not os.path.exists(img_path):\n",
    "            continue\n",
    "            \n",
    "        # Generate caption\n",
    "        generated_caption = generate_caption(model, img_path, tokenizer, transform, device=device)\n",
    "        \n",
    "        # Process reference captions\n",
    "        reference_captions = []\n",
    "        for caption in captions:\n",
    "            # Clean up reference caption - remove <start> and <end>\n",
    "            clean_caption = caption.replace(\"<start>\", \"\").replace(\"<end>\", \"\").strip()\n",
    "            reference_captions.append(clean_caption.split())\n",
    "        \n",
    "        # Process generated caption\n",
    "        generated_caption = generated_caption.split()\n",
    "        \n",
    "        # Add to lists for corpus calculation\n",
    "        references.append(reference_captions)\n",
    "        hypotheses.append(generated_caption)\n",
    "    \n",
    "    # Calculate BLEU scores\n",
    "    bleu1 = corpus_bleu(references, hypotheses, weights=(1, 0, 0, 0))\n",
    "    bleu2 = corpus_bleu(references, hypotheses, weights=(0.5, 0.5, 0, 0))\n",
    "    bleu3 = corpus_bleu(references, hypotheses, weights=(0.33, 0.33, 0.33, 0))\n",
    "    bleu4 = corpus_bleu(references, hypotheses, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "    \n",
    "    return {\n",
    "        'BLEU-1': bleu1 * 100,\n",
    "        'BLEU-2': bleu2 * 100,\n",
    "        'BLEU-3': bleu3 * 100,\n",
    "        'BLEU-4': bleu4 * 100\n",
    "    }\n",
    "\n",
    "# Update in your main function:\n",
    "def main():\n",
    "    # Prepare NLTK data\n",
    "    prepare_nltk_data()\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Load model\n",
    "    checkpoint_path = \"CA2_checkpoint_epoch_30.pth\"  # Update with your checkpoint path\n",
    "    model = load_model(checkpoint_path, ImageCaptioningModelWithCrossAttention, device)\n",
    "    \n",
    "    try:\n",
    "        # Try to evaluate with BLEU and METEOR\n",
    "        metrics = evaluate_model(model, test_data, tokenizer, transform, device=device)\n",
    "    except LookupError as e:\n",
    "        print(f\"Error with full evaluation: {e}\")\n",
    "        print(\"Falling back to BLEU-only evaluation\")\n",
    "        # Fall back to BLEU-only evaluation\n",
    "        metrics = evaluate_model_bleu_only(model, test_data, tokenizer, transform, device=device)\n",
    "    print(\"Evaluation Metrics:\")\n",
    "    for metric_name, score in metrics.items():\n",
    "        print(f\"{metric_name}: {score:.2f}%\")\n",
    "    \n",
    "    # Visualize some examples\n",
    "    visualize_examples(model, test_data, tokenizer, transform, num_examples=5, device=device)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d44578f",
   "metadata": {},
   "source": [
    "#### BLEU score of 32.53% and METEOR score of ~31% is not bad. But comparing to our previous model, it performs slighly worse. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4c1c4e-15bc-49c9-aade-ce1229f1401a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437fb421-761c-4977-adcb-67f7f928026f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33af2756-c367-45a3-880c-c78825de396f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def caption_single_image(image_path, model, tokenizer, transform, max_length=30, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Generate caption for a single custom image\n",
    "    \"\"\"\n",
    "    # Load and preprocess the image\n",
    "    try:\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "        \n",
    "        # Set model to evaluation mode\n",
    "        model.eval()\n",
    "        \n",
    "        # Start with BERT's [CLS] token (equivalent to <start> in your training)\n",
    "        start_token = tokenizer.cls_token_id\n",
    "        caption = torch.tensor([[start_token]]).to(device)\n",
    "        \n",
    "        # Generate caption token by token\n",
    "        with torch.no_grad():\n",
    "            # Get image features\n",
    "            image_features = model.image_encoder(image_tensor)\n",
    "            \n",
    "            # Generate tokens one by one\n",
    "            for _ in range(max_length):\n",
    "                # Get model prediction\n",
    "                output = model.transformer_decoder(image_features, caption)\n",
    "                \n",
    "                # Get the next word prediction (last token)\n",
    "                predicted = output[:, -1, :].argmax(dim=1, keepdim=True)\n",
    "                \n",
    "                # Append the predicted token to the caption\n",
    "                caption = torch.cat([caption, predicted], dim=1)\n",
    "                \n",
    "                # Stop if end token is predicted (BERT's [SEP] token)\n",
    "                if predicted.item() == tokenizer.sep_token_id:\n",
    "                    break\n",
    "        \n",
    "        # Convert tokens to text\n",
    "        caption_text = tokenizer.decode(caption[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Display the image and caption\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(image)\n",
    "        plt.title(f\"Caption: {caption_text}\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        \n",
    "        return caption_text\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Usage example:\n",
    "# First load your model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "checkpoint_path = \"CA2_checkpoint_epoch_30.pth\"\n",
    "model = load_model(checkpoint_path, ImageCaptioningModelWithCrossAttention, device)\n",
    "#\n",
    "# Then generate caption for your image\n",
    "test_image_path = \"generate_test.JPG\"\n",
    "caption = caption_single_image(test_image_path, model, tokenizer, transform, device=device)\n",
    "print(f\"Generated caption: {caption}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36611525",
   "metadata": {},
   "source": [
    "#### If we read generated caption from both models, we can feel model 1 did a better prediction. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be62794d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7f49ce20",
   "metadata": {},
   "source": [
    "#### To improve the performance, let's try few things: use early stop, weight decay, learning rate schdular, dropout. These are standard approaches for tackle overfitting. Let's see if they improve the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1807f202",
   "metadata": {},
   "source": [
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b462941c-71a2-4e04-be4d-8520fec7f4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttentionTransformerDecoder(nn.Module):\n",
    "    def __init__(self, embed_size, vocab_size, embedding_dim, nhead, num_layers):\n",
    "        super(CrossAttentionTransformerDecoder, self).__init__()\n",
    "        \n",
    "        # Embedding layer for captions\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.embed_size = embedding_dim\n",
    "        max_len = 100  # Maximum sequence length\n",
    "        pe = torch.zeros(max_len, embedding_dim)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, embedding_dim, 2).float() * -(torch.log(torch.tensor(10000.0)) / embedding_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))  # Shape: [1, max_len, embedding_dim]\n",
    "        \n",
    "        # Linear layer to project image features to match decoder dimensions\n",
    "        self.img_projection = nn.Linear(embed_size, embedding_dim)\n",
    "        \n",
    "        # Custom transformer decoder layers with cross-attention\n",
    "        self.decoder_layers = nn.ModuleList([\n",
    "            DecoderLayerWithCrossAttention(embedding_dim, nhead)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Output linear layer\n",
    "        self.fc_out = nn.Linear(embedding_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, image_features, captions):\n",
    "        # Embedding captions\n",
    "        embedded = self.embedding(captions)  # Shape: [batch_size, seq_len, embedding_dim]\n",
    "        \n",
    "        # Add positional encoding\n",
    "        seq_len = embedded.size(1)\n",
    "        positional_encoding = self.pe[:, :seq_len, :]  # Shape: [1, seq_len, embedding_dim]\n",
    "        embedded = embedded + positional_encoding\n",
    "        \n",
    "        # Project image features to match embedding dimension\n",
    "        image_features = self.img_projection(image_features)  # Shape: [batch_size, embedding_dim]\n",
    "        \n",
    "        # Create self-attention mask to prevent attention to future tokens\n",
    "        tgt_mask = self.generate_square_subsequent_mask(seq_len).to(embedded.device)\n",
    "        \n",
    "        # Process through decoder layers\n",
    "        output = embedded\n",
    "        for decoder_layer in self.decoder_layers:\n",
    "            output = decoder_layer(output, image_features, tgt_mask)\n",
    "        \n",
    "        # Project to vocabulary size\n",
    "        output = self.fc_out(output)  # Shape: [batch_size, seq_len, vocab_size]\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        # Generate a square mask for the sequence to prevent attending to future tokens\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "\n",
    "class DecoderLayerWithCrossAttention(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dropout=0.3):\n",
    "        super(DecoderLayerWithCrossAttention, self).__init__()\n",
    "        \n",
    "        # Self-attention\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        \n",
    "        # Cross-attention between text and image features\n",
    "        self.cross_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        \n",
    "        # Feed-forward network\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, 4 * d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(4 * d_model, d_model)\n",
    "        )\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, tgt, memory, tgt_mask=None):\n",
    "        # Self-attention\n",
    "        tgt2 = self.self_attn(\n",
    "            query=tgt.transpose(0, 1),\n",
    "            key=tgt.transpose(0, 1),\n",
    "            value=tgt.transpose(0, 1),\n",
    "            attn_mask=tgt_mask\n",
    "        )[0].transpose(0, 1)\n",
    "        tgt = self.norm1(tgt + self.dropout1(tgt2))\n",
    "        \n",
    "        # Cross-attention between text and image\n",
    "        # Expand image features to be compatible with text sequence\n",
    "        batch_size, seq_len, d_model = tgt.size()\n",
    "        expanded_memory = memory.unsqueeze(1).expand(-1, seq_len, -1)  # [batch_size, seq_len, d_model]\n",
    "        \n",
    "        # Prepare for cross-attention\n",
    "        q = tgt.transpose(0, 1)  # [seq_len, batch_size, d_model]\n",
    "        k = expanded_memory.transpose(0, 1)  # [seq_len, batch_size, d_model]\n",
    "        v = expanded_memory.transpose(0, 1)  # [seq_len, batch_size, d_model]\n",
    "        \n",
    "        # Apply cross-attention\n",
    "        tgt2 = self.cross_attn(q, k, v)[0].transpose(0, 1)\n",
    "        tgt = self.norm2(tgt + self.dropout2(tgt2))\n",
    "        \n",
    "        # Feed-forward network\n",
    "        tgt2 = self.ffn(tgt)\n",
    "        tgt = self.norm3(tgt + self.dropout3(tgt2))\n",
    "        \n",
    "        return tgt\n",
    "\n",
    "class ImageCaptioningModelWithCrossAttention(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, embedding_dim, nhead, num_layers):\n",
    "        super(ImageCaptioningModelWithCrossAttention, self).__init__()\n",
    "        \n",
    "        self.image_encoder = ImageEncoder(embed_size)\n",
    "        self.transformer_decoder = CrossAttentionTransformerDecoder(\n",
    "            embed_size=embed_size,\n",
    "            vocab_size=vocab_size,\n",
    "            embedding_dim=embedding_dim,\n",
    "            nhead=nhead,\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        \n",
    "    def forward(self, images, captions):\n",
    "        image_features = self.image_encoder(images)\n",
    "        output = self.transformer_decoder(image_features, captions)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3843123-260a-4f0b-b22d-16fd259f5059",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Change the training to lower overfitting: use early stop, weight decay, learning rate schdular. \n",
    "\n",
    "def train(model, train_loader, val_loader, criterion, optimizer, num_epochs, device, vocab_size):\n",
    "    # Lists to store losses for plotting\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    # Early stopping parameters\n",
    "    best_val_loss = float('inf')\n",
    "    early_stopping_counter = 0\n",
    "    early_stopping_patience = 5\n",
    "    \n",
    "    # Create learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', patience=3, factor=0.5, verbose=True\n",
    "    )\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        \n",
    "        for images, captions in tqdm(iter(train_loader), total=len(train_loader)):\n",
    "            images = images.to(device)\n",
    "            captions = captions.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass (exclude last token from input)\n",
    "            outputs = model(images, captions[:, :-1])\n",
    "            \n",
    "            # Reshape outputs and captions for loss calculation\n",
    "            outputs = outputs.contiguous().view(-1, vocab_size)\n",
    "            targets = captions[:, 1:].contiguous().view(-1)  # Skip the first token (usually BOS)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "             \n",
    "            # Clip gradients to avoid exploding gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            total_train_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        \n",
    "        # Validate the model\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for images, captions in tqdm(iter(val_loader), total=len(val_loader)):\n",
    "                images = images.to(device)\n",
    "                captions = captions.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(images, captions[:, :-1])\n",
    "                \n",
    "                # Reshape outputs and captions for loss calculation\n",
    "                outputs = outputs.contiguous().view(-1, vocab_size)\n",
    "                targets = captions[:, 1:].contiguous().view(-1)\n",
    "                \n",
    "                # Calculate loss\n",
    "                loss = criterion(outputs, targets)\n",
    "                total_val_loss += loss.item()\n",
    "        \n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        \n",
    "        # Update learning rate scheduler based on validation loss\n",
    "        scheduler.step(avg_val_loss)\n",
    "        \n",
    "        # Save the losses for plotting\n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "        \n",
    "        # Check for best model and save it\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            early_stopping_counter = 0\n",
    "            # Save best model\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': avg_train_loss,\n",
    "                'val_loss': avg_val_loss,\n",
    "            }, \"CA2_best_model_modifed.pth\")\n",
    "            print(f\"New best model saved with validation loss: {avg_val_loss:.4f}\")\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "            print(f\"Validation loss did not improve. Early stopping counter: {early_stopping_counter}/{early_stopping_patience}\")\n",
    "        \n",
    "        # Regular checkpoint saving\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': avg_train_loss,\n",
    "                'val_loss': avg_val_loss,\n",
    "            }, f\"CA2_checkpoint_epoch_{epoch+1}_modified.pth\")\n",
    "        \n",
    "        # Early stopping check\n",
    "        if early_stopping_counter >= early_stopping_patience:\n",
    "            print(f\"Early stopping triggered after epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    return train_losses, val_losses\n",
    "\n",
    "# Initialize model parameters\n",
    "embed_size = 512        # Embedding size for image features\n",
    "embedding_dim = 512     # Embedding dimension for captions\n",
    "nhead = 8               # Number of attention heads\n",
    "num_layers = 6          # Number of transformer layers\n",
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "# Create the image captioning model\n",
    "model = ImageCaptioningModelWithCrossAttention(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_size=embed_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    nhead=nhead,\n",
    "    num_layers=num_layers\n",
    ")\n",
    "# Move model to device (GPU if available, otherwise CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Define loss function and optimizer with weight decay for regularization\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5, weight_decay=1e-5)  # Lower learning rate and add weight decay\n",
    "\n",
    "# Now you can start training with the improved train function\n",
    "train_losses, val_losses = train(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    num_epochs=30,\n",
    "    device=device,\n",
    "    vocab_size=vocab_size\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b4466c-c755-488b-afec-fbc49a48e296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training, you can plot the losses\n",
    "import matplotlib.pyplot as plt\n",
    "num_epochs=30\n",
    "plt.plot(range(1, num_epochs+1), train_losses, label='Training Loss')\n",
    "plt.plot(range(1, num_epochs+1), val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.savefig('training_curve_cross_attention_2.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224fb835-d23d-4fcc-8e42-6713cab34d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update in your main function:\n",
    "def visualize_examples(model, test_data, tokenizer, transform, num_examples=5, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Visualize some example predictions\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import random\n",
    "    \n",
    "    # Select random images from test data\n",
    "    img_paths = list(test_data.keys())\n",
    "    selected_paths = random.sample(img_paths, min(num_examples, len(img_paths)))\n",
    "    \n",
    "    fig, axes = plt.subplots(num_examples, 1, figsize=(15, 5*num_examples))\n",
    "    \n",
    "    for i, img_path in enumerate(selected_paths):\n",
    "        if not os.path.exists(img_path):\n",
    "            continue\n",
    "            \n",
    "        # Generate caption\n",
    "        generated_caption = generate_caption(model, img_path, tokenizer, transform, device=device)\n",
    "        \n",
    "        # Reference captions\n",
    "        reference_captions = [caption.replace(\"<start>\", \"\").replace(\"<end>\", \"\").strip() \n",
    "                             for caption in test_data[img_path]]\n",
    "        \n",
    "        # Display image and captions\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        if num_examples > 1:\n",
    "            ax = axes[i]\n",
    "        else:\n",
    "            ax = axes\n",
    "            \n",
    "        ax.imshow(img)\n",
    "        ax.set_title(f\"Generated: {generated_caption}\")\n",
    "        caption_text = \"\\n\".join([f\"Reference {j+1}: {ref}\" for j, ref in enumerate(reference_captions)])\n",
    "        ax.set_xlabel(caption_text)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"caption_examples_cross_attention2.png\")\n",
    "    plt.show()\n",
    "\n",
    "def main():\n",
    "    # Prepare NLTK data\n",
    "    prepare_nltk_data()\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Load model\n",
    "    checkpoint_path = \"CA2_best_model_modifed.pth\"  # Update with your checkpoint path\n",
    "    model = load_model(checkpoint_path, ImageCaptioningModelWithCrossAttention, device)\n",
    "    \n",
    "    try:\n",
    "        # Try to evaluate with BLEU and METEOR\n",
    "        metrics = evaluate_model(model, test_data, tokenizer, transform, device=device)\n",
    "    except LookupError as e:\n",
    "        print(f\"Error with full evaluation: {e}\")\n",
    "        print(\"Falling back to BLEU-only evaluation\")\n",
    "        # Fall back to BLEU-only evaluation\n",
    "        metrics = evaluate_model_bleu_only(model, test_data, tokenizer, transform, device=device)\n",
    "    print(\"Evaluation Metrics:\")\n",
    "    for metric_name, score in metrics.items():\n",
    "        print(f\"{metric_name}: {score:.2f}%\")\n",
    "    \n",
    "    # Visualize some examples\n",
    "    visualize_examples(model, test_data, tokenizer, transform, num_examples=5, device=device)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8bee21",
   "metadata": {},
   "source": [
    "#### Again the BLEU score and METEOR score do not seem to improve, in fact they are still worse compared to the our original model without cross attention. The reason could be that as we are incorporating cross-attention, we are making the model more complicated and harder to train. So perhaps we need to train it for a higher epochs or need to have a larger dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b832619-bc8e-4fbf-9ce0-b98c3cb558d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def caption_single_image(image_path, model, tokenizer, transform, max_length=30, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Generate caption for a single custom image\n",
    "    \"\"\"\n",
    "    # Load and preprocess the image\n",
    "    try:\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "        \n",
    "        # Set model to evaluation mode\n",
    "        model.eval()\n",
    "        \n",
    "        # Start with BERT's [CLS] token (equivalent to <start> in your training)\n",
    "        start_token = tokenizer.cls_token_id\n",
    "        caption = torch.tensor([[start_token]]).to(device)\n",
    "        \n",
    "        # Generate caption token by token\n",
    "        with torch.no_grad():\n",
    "            # Get image features\n",
    "            image_features = model.image_encoder(image_tensor)\n",
    "            \n",
    "            # Generate tokens one by one\n",
    "            for _ in range(max_length):\n",
    "                # Get model prediction\n",
    "                output = model.transformer_decoder(image_features, caption)\n",
    "                \n",
    "                # Get the next word prediction (last token)\n",
    "                predicted = output[:, -1, :].argmax(dim=1, keepdim=True)\n",
    "                \n",
    "                # Append the predicted token to the caption\n",
    "                caption = torch.cat([caption, predicted], dim=1)\n",
    "                \n",
    "                # Stop if end token is predicted (BERT's [SEP] token)\n",
    "                if predicted.item() == tokenizer.sep_token_id:\n",
    "                    break\n",
    "        \n",
    "        # Convert tokens to text\n",
    "        caption_text = tokenizer.decode(caption[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Display the image and caption\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(image)\n",
    "        plt.title(f\"Caption: {caption_text}\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        \n",
    "        return caption_text\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Usage example:\n",
    "# First load your model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "checkpoint_path = \"CA2_best_model_modifed.pth\"\n",
    "model = load_model(checkpoint_path, ImageCaptioningModelWithCrossAttention, device)\n",
    "#\n",
    "# Then generate caption for your image\n",
    "test_image_path = \"generate_test.JPG\"\n",
    "caption = caption_single_image(test_image_path, model, tokenizer, transform, device=device)\n",
    "print(f\"Generated caption: {caption}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9ae4bf",
   "metadata": {},
   "source": [
    "#### All three version of models are trained and evaluated. Let's plot their performances for comparision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3e0390",
   "metadata": {},
   "source": [
    "CA_mod=Evaluation Metrics:\n",
    "BLEU-1: 32.79%\n",
    "BLEU-2: 20.43%\n",
    "BLEU-3: 12.28%\n",
    "BLEU-4: 6.78%\n",
    "METEOR: 29.74%\n",
    "\n",
    "CA=Evaluation Metrics:\n",
    "BLEU-1: 32.53%\n",
    "BLEU-2: 20.72%\n",
    "BLEU-3: 12.54%\n",
    "BLEU-4: 6.87%\n",
    "METEOR: 30.94%\n",
    "\n",
    "no_CA=Evaluation Metrics:\n",
    "BLEU-1: 33.76%\n",
    "BLEU-2: 20.96%\n",
    "BLEU-3: 13.31%\n",
    "BLEU-4: 7.86%\n",
    "METEOR: 31.68%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c618626a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Data\n",
    "methods = ['No-CA', 'CA', 'CA-modified']\n",
    "metrics = ['BLEU-1', 'BLEU-2', 'BLEU-3', 'BLEU-4', 'METEOR']\n",
    "\n",
    "# Scores for each method\n",
    "scores = {\n",
    "    'No-CA': [33.76, 20.96, 13.31, 7.86, 31.68],\n",
    "    'CA': [32.53, 20.72, 12.54, 6.87, 30.94],\n",
    "    'CA-modified': [32.79, 20.43, 12.28, 6.78, 29.74]\n",
    "}\n",
    "\n",
    "# Setup for plotting\n",
    "x = np.arange(len(metrics))  # Metric positions\n",
    "width = 0.25  # Width of bars\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plot each method\n",
    "ax.bar(x - width, scores['No-CA'], width, label='No-CA')\n",
    "ax.bar(x, scores['CA'], width, label='CA')\n",
    "ax.bar(x + width, scores['CA-modified'], width, label='CA-modified')\n",
    "\n",
    "# Labels and titles\n",
    "ax.set_xlabel('Evaluation Metric')\n",
    "ax.set_ylabel('Score (%)')\n",
    "ax.set_title('Comparison of Evaluation Metrics for Captioning Methods')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(metrics)\n",
    "ax.legend()\n",
    "\n",
    "# Display scores on bars\n",
    "for i, method in enumerate(methods):\n",
    "    offset = (i - 1) * width\n",
    "    for j, val in enumerate(scores[method]):\n",
    "        ax.text(x[j] + offset, val + 0.5, f'{val:.2f}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('Overall_performance.pdf')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624d7eb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0052a734-c819-4a76-afc0-a2336354b867",
   "metadata": {},
   "source": [
    "### 3. Let's perform the Image-text retrieval task now as we will use same Flickr30K dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf5481c-ae71-4345-9338-af46468c9381",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here we need to use dual encoder- one for image and another for text and encode them in the same place. \n",
    "## The model try to minimize the contrast between a pair of image and text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34eaff54-fa2f-458c-8c84-9a298d9bc1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Encoder - Using a pre-trained CNN\n",
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(ImageEncoder, self).__init__()\n",
    "        # Load pre-trained ResNet model\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        # Remove the last fully connected layer\n",
    "        modules = list(resnet.children())[:-1]\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        # Add a projection layer to get embedding_dim size\n",
    "        self.fc = nn.Linear(2048, embedding_dim)\n",
    "        \n",
    "    def forward(self, images):\n",
    "        with torch.no_grad():\n",
    "            features = self.resnet(images)\n",
    "        features = features.view(features.size(0), -1)  # Flatten\n",
    "        features = self.fc(features)\n",
    "        # L2 normalize embeddings\n",
    "        features = F.normalize(features, p=2, dim=1)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4201e02-5525-476b-80c7-e52c80b4fe99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Encoder - Using pre-trained BERT\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(TextEncoder, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        # Freeze BERT parameters for efficiency\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "        # Add a projection layer to get embedding_dim size\n",
    "        self.fc = nn.Linear(768, embedding_dim)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        with torch.no_grad():\n",
    "            outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            # Use the [CLS] token embedding as the text representation\n",
    "            text_features = outputs.last_hidden_state[:, 0, :]\n",
    "        text_features = self.fc(text_features)\n",
    "        # L2 normalize embeddings\n",
    "        text_features = F.normalize(text_features, p=2, dim=1)\n",
    "        return text_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e52b589-7520-44ed-b13a-861c2c6a2d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dual Encoder for Image-Text Retrieval\n",
    "class DualEncoder(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(DualEncoder, self).__init__()\n",
    "        self.image_encoder = ImageEncoder(embedding_dim)\n",
    "        self.text_encoder = TextEncoder(embedding_dim)\n",
    "        \n",
    "    def forward(self, images=None, input_ids=None, attention_mask=None):\n",
    "        image_features = None\n",
    "        text_features = None\n",
    "        \n",
    "        if images is not None:\n",
    "            image_features = self.image_encoder(images)\n",
    "            \n",
    "        if input_ids is not None and attention_mask is not None:\n",
    "            text_features = self.text_encoder(input_ids, attention_mask)\n",
    "            \n",
    "        return image_features, text_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3785bee-6d5b-4b31-a61c-261d7b8f49d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset for Image-Text Retrieval\n",
    "class Flickr30kRetrievalDataset(Dataset):\n",
    "    def __init__(self, data_dict, transform=None, tokenizer=None):\n",
    "        self.data_dict = data_dict\n",
    "        self.transform = transform\n",
    "        self.tokenizer = tokenizer\n",
    "        self.image_paths = list(data_dict.keys())\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        captions = self.data_dict[image_path]\n",
    "        \n",
    "        # Randomly select one caption\n",
    "        caption = random.choice(captions)\n",
    "        \n",
    "        # Load and transform the image\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # Tokenize the caption\n",
    "        tokenized = self.tokenizer(\n",
    "            caption,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=SEQ_LENGTH,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        input_ids = tokenized['input_ids'].squeeze(0)\n",
    "        attention_mask = tokenized['attention_mask'].squeeze(0)\n",
    "        \n",
    "        return image, input_ids, attention_mask, idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646c8b95-30e8-466b-89a0-7628b1350e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contrastive Loss Function for Retrieval\n",
    "class NTXentLoss(nn.Module):\n",
    "    def __init__(self, temperature=0.07):\n",
    "        super(NTXentLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, image_features, text_features):\n",
    "        # Compute similarity matrix\n",
    "        logits = torch.matmul(image_features, text_features.T) / self.temperature\n",
    "        \n",
    "        # Labels: diagonal elements (matching pairs)\n",
    "        labels = torch.arange(logits.size(0)).to(logits.device)\n",
    "        \n",
    "        # Compute loss in both directions\n",
    "        i2t_loss = self.criterion(logits, labels)  # image-to-text\n",
    "        t2i_loss = self.criterion(logits.T, labels)  # text-to-image\n",
    "        \n",
    "        # Return average of the bidirectional loss\n",
    "        return (i2t_loss + t2i_loss) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aaf4f89-adb6-426a-8d6a-4999d40b0f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_retrieval_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device):\n",
    "    best_val_loss = float('inf')\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        \n",
    "        for images, input_ids, attention_mask, _ in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\"):\n",
    "            images = images.to(device)\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            image_features, text_features = model(images, input_ids, attention_mask)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(image_features, text_features)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_train_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, input_ids, attention_mask, _ in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\"):\n",
    "                images = images.to(device)\n",
    "                input_ids = input_ids.to(device)\n",
    "                attention_mask = attention_mask.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                image_features, text_features = model(images, input_ids, attention_mask)\n",
    "                \n",
    "                # Compute loss\n",
    "                loss = criterion(image_features, text_features)\n",
    "                \n",
    "                total_val_loss += loss.item()\n",
    "        \n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "        \n",
    "        # Save the best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': best_val_loss,\n",
    "            }, 'best_retrieval_model.pth')\n",
    "            \n",
    "        # Save checkpoint every 5 epochs\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': avg_train_loss,\n",
    "                'val_loss': avg_val_loss,\n",
    "            }, f'retrieval_model_epoch_{epoch+1}.pth')\n",
    "    \n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff05562e-9f14-4e78-9c60-9505466b4fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_retrieval(model, test_loader, device):\n",
    "    model.eval()\n",
    "    \n",
    "    # Lists to store all features\n",
    "    all_image_features = []\n",
    "    all_text_features = []\n",
    "    all_indices = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, input_ids, attention_mask, indices in tqdm(test_loader, desc=\"Extracting features\"):\n",
    "            images = images.to(device)\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            \n",
    "            # Get features\n",
    "            image_features, text_features = model(images, input_ids, attention_mask)\n",
    "            \n",
    "            all_image_features.append(image_features.cpu())\n",
    "            all_text_features.append(text_features.cpu())\n",
    "            all_indices.extend(indices.tolist())\n",
    "    \n",
    "    # Concatenate all features\n",
    "    all_image_features = torch.cat(all_image_features, dim=0)\n",
    "    all_text_features = torch.cat(all_text_features, dim=0)\n",
    "    \n",
    "    # Compute similarity matrix\n",
    "    similarity = torch.matmul(all_image_features, all_text_features.T)\n",
    "    \n",
    "    # Compute metrics\n",
    "    # Image-to-Text retrieval\n",
    "    i2t_ranks = []\n",
    "    for i in range(similarity.size(0)):\n",
    "        # Get ranking of correct caption\n",
    "        ranking = (similarity[i, :] > similarity[i, i]).sum().item() + 1\n",
    "        i2t_ranks.append(ranking)\n",
    "    \n",
    "    # Text-to-Image retrieval\n",
    "    t2i_ranks = []\n",
    "    for i in range(similarity.size(1)):\n",
    "        # Get ranking of correct image\n",
    "        ranking = (similarity[:, i] > similarity[i, i]).sum().item() + 1\n",
    "        t2i_ranks.append(ranking)\n",
    "    \n",
    "    # Calculate recall metrics\n",
    "    def recall_at_k(ranks, k):\n",
    "        return len([r for r in ranks if r <= k]) / len(ranks)\n",
    "    \n",
    "    i2t_r1 = recall_at_k(i2t_ranks, 1)\n",
    "    i2t_r5 = recall_at_k(i2t_ranks, 5)\n",
    "    i2t_r10 = recall_at_k(i2t_ranks, 10)\n",
    "    \n",
    "    t2i_r1 = recall_at_k(t2i_ranks, 1)\n",
    "    t2i_r5 = recall_at_k(t2i_ranks, 5)\n",
    "    t2i_r10 = recall_at_k(t2i_ranks, 10)\n",
    "    \n",
    "    # Median rank\n",
    "    i2t_median_rank = np.median(i2t_ranks)\n",
    "    t2i_median_rank = np.median(t2i_ranks)\n",
    "    \n",
    "    results = {\n",
    "        \"Image-to-Text\": {\n",
    "            \"R@1\": i2t_r1,\n",
    "            \"R@5\": i2t_r5,\n",
    "            \"R@10\": i2t_r10,\n",
    "            \"Median Rank\": i2t_median_rank\n",
    "        },\n",
    "        \"Text-to-Image\": {\n",
    "            \"R@1\": t2i_r1,\n",
    "            \"R@5\": t2i_r5,\n",
    "            \"R@10\": t2i_r10,\n",
    "            \"Median Rank\": t2i_median_rank\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return results, similarity, all_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0752755a-422c-4cd7-8629-b2b43aa42dcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3693f2a9-c150-4efb-871c-ac72aec0b0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loaders(train_data, validation_data, test_data, batch_size=32):\n",
    "    # Image transformation\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(IMAGE_SIZE),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet stats\n",
    "    ])\n",
    "    \n",
    "    # Initialize BERT tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = Flickr30kRetrievalDataset(train_data, transform, tokenizer)\n",
    "    val_dataset = Flickr30kRetrievalDataset(validation_data, transform, tokenizer)\n",
    "    test_dataset = Flickr30kRetrievalDataset(test_data, transform, tokenizer)\n",
    "    \n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea2ed3d-d269-406a-b9d9-309985041b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM=512\n",
    "# Initialize model and optimization components\n",
    "def initialize_model(embedding_dim=EMBEDDING_DIM, lr=1e-4):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Create model\n",
    "    model = DualEncoder(embedding_dim=embedding_dim).to(device)\n",
    "    \n",
    "    # Define loss and optimizer\n",
    "    criterion = NTXentLoss(temperature=0.07)\n",
    "    optimizer = torch.optim.Adam(\n",
    "        [p for p in model.parameters() if p.requires_grad],\n",
    "        lr=lr,\n",
    "        weight_decay=1e-5\n",
    "    )\n",
    "    \n",
    "    return model, criterion, optimizer, device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f61f895-c732-48dc-8504-c5b94f84a747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot and save training curves\n",
    "def plot_training_curves(train_losses, val_losses, filename='retrieval_loss.png'):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(filename)\n",
    "    plt.show()\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f0f405-cb6b-412d-b32c-272eb16cc72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print and save evaluation results\n",
    "def print_and_save_results(results, similarity_matrix=None, indices=None):\n",
    "    print(\"\\nRetrieval Results:\")\n",
    "    print(\"Image-to-Text:\")\n",
    "    print(f\"R@1: {results['Image-to-Text']['R@1']:.4f}\")\n",
    "    print(f\"R@5: {results['Image-to-Text']['R@5']:.4f}\")\n",
    "    print(f\"R@10: {results['Image-to-Text']['R@10']:.4f}\")\n",
    "    print(f\"Median Rank: {results['Image-to-Text']['Median Rank']}\")\n",
    "    \n",
    "    print(\"\\nText-to-Image:\")\n",
    "    print(f\"R@1: {results['Text-to-Image']['R@1']:.4f}\")\n",
    "    print(f\"R@5: {results['Text-to-Image']['R@5']:.4f}\")\n",
    "    print(f\"R@10: {results['Text-to-Image']['R@10']:.4f}\")\n",
    "    print(f\"Median Rank: {results['Text-to-Image']['Median Rank']}\")\n",
    "    \n",
    "    # Save results\n",
    "    np.save('retrieval_results.npy', results)\n",
    "    if similarity_matrix is not None:\n",
    "        np.save('similarity_matrix.npy', similarity_matrix.numpy())\n",
    "    if indices is not None:\n",
    "        np.save('test_indices.npy', np.array(indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0884374b-91d1-46e8-b282-11d42336f83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(train_model=True, evaluate_model=True, batch_size=32, num_epochs=20, lr=1e-4):\n",
    "    # Prepare data\n",
    "    #train_data, validation_data, test_data, text_data = prepare_data()\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader, val_loader, test_loader, tokenizer = create_data_loaders(\n",
    "        train_data, validation_data, test_data, batch_size\n",
    "    )\n",
    "    \n",
    "    # Initialize model\n",
    "    model, criterion, optimizer, device = initialize_model(EMBEDDING_DIM, lr)\n",
    "    \n",
    "    # Train model (optional)\n",
    "    if train_model:\n",
    "        train_losses, val_losses = train_retrieval_model(\n",
    "            model, train_loader, val_loader, criterion, optimizer, num_epochs, device\n",
    "        )\n",
    "        plot_training_curves(train_losses, val_losses)\n",
    "    \n",
    "    # Load the best model (if available and evaluation is requested)\n",
    "    if evaluate_model:\n",
    "        try:\n",
    "            checkpoint = torch.load('best_retrieval_model.pth')\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            print(\"Loaded best model checkpoint\")\n",
    "        except FileNotFoundError:\n",
    "            print(\"No checkpoint found, using current model state\")\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        results, similarity_matrix, indices = evaluate_retrieval(model, test_loader, device)\n",
    "        print_and_save_results(results, similarity_matrix, indices)\n",
    "    \n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4931104-2763-42b9-a289-e258096ada41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_only(batch_size=32, num_epochs=20, lr=1e-4):\n",
    "    model, tokenizer = main(train_model=True, evaluate_model=False, batch_size=batch_size, num_epochs=num_epochs, lr=lr)\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835a3373-bbea-4ff2-857f-31ff8adc905e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_only(model_path='best_retrieval_model.pth', batch_size=32):\n",
    "    # Load data\n",
    "    #train_data, validation_data, test_data, text_data = prepare_data()\n",
    "    \n",
    "    # Create test loader only\n",
    "    _, _, test_loader, tokenizer = create_data_loaders(\n",
    "        train_data, validation_data, test_data, batch_size\n",
    "    )\n",
    "    \n",
    "    # Initialize model\n",
    "    model, _, _, device = initialize_model()\n",
    "    \n",
    "    # Load model weights\n",
    "    checkpoint = torch.load(model_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    # Evaluate\n",
    "    results, similarity_matrix, indices = evaluate_retrieval(model, test_loader, device)\n",
    "    print_and_save_results(results, similarity_matrix, indices)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77d4885-f2ca-40b2-8dbb-3fca2ccee24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca50208-334a-47f0-ab87-ff817b5150aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    model, tokenizer=train_only()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d56ccc-2357-4a4b-8fa7-20bff18e0735",
   "metadata": {},
   "outputs": [],
   "source": [
    "#forgot to show the trainign curve but saved it. therefore, will show it here\n",
    "#import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "img = mpimg.imread('retrieval_loss.png')  # replace with your file name\n",
    "plt.imshow(img)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b71b9b-e9b1-4b8e-ab52-86fce8d3c661",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2d8263-9ff6-41ba-8152-98ec1cbe19b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8345e72b-d5f1-4b20-86d5-1b7e9a36f9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    results=evaluate_only()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99474ad4-3531-4380-9c2d-66e443030a62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90b9af0-d304-45a7-b116-3a87e114d912",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torchvision.transforms import ToPILImage\n",
    "\n",
    "def demo_retrieval_with_test_loader(model, test_loader, tokenizer, device, k=5):\n",
    "    \"\"\"\n",
    "    Demonstrate retrieval using samples from the test loader.\n",
    "    Adjusted for a dataset that returns (images, captions) pairs.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained dual encoder model\n",
    "        test_loader: DataLoader for test data\n",
    "        tokenizer: BERT tokenizer\n",
    "        device: Device to run model on\n",
    "        k: Number of top matches to show\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Get a batch from the test loader\n",
    "    for images, captions in test_loader:\n",
    "        # Move to device\n",
    "        images = images.to(device)\n",
    "        \n",
    "        # Check what kind of caption format we have\n",
    "        if isinstance(captions, torch.Tensor):\n",
    "            # If captions is just token IDs without attention mask\n",
    "            input_ids = captions.to(device)\n",
    "            attention_mask = torch.ones_like(input_ids).to(device)\n",
    "        else:\n",
    "            # If your loader returns something else, adjust accordingly\n",
    "            print(f\"Unexpected caption format: {type(captions)}\")\n",
    "            continue\n",
    "        \n",
    "        # Get all features\n",
    "        with torch.no_grad():\n",
    "            image_features, text_features = model(images, input_ids, attention_mask)\n",
    "        \n",
    "        # Convert features to CPU for processing\n",
    "        image_features = image_features.cpu()\n",
    "        text_features = text_features.cpu()\n",
    "        \n",
    "        break  # We only need one batch\n",
    "    \n",
    "    # Compute full similarity matrix\n",
    "    similarity = torch.matmul(image_features, text_features.T)\n",
    "    \n",
    "    # Get a few random indices for demo\n",
    "    num_samples = min(5, len(images))\n",
    "    image_indices = random.sample(range(len(images)), num_samples)\n",
    "    text_indices = random.sample(range(len(input_ids)), num_samples)\n",
    "    \n",
    "    # Demo 1: Image to Text Retrieval\n",
    "    print(\"\\n===== IMAGE TO TEXT RETRIEVAL =====\")\n",
    "    for idx in image_indices:\n",
    "        # Get an image\n",
    "        img = images[idx].cpu()\n",
    "        \n",
    "        # Convert to PIL Image for display\n",
    "        img_pil = ToPILImage()(img)\n",
    "        \n",
    "        # Get similarity scores for this image with all texts\n",
    "        img_text_sim = similarity[idx]\n",
    "        \n",
    "        # Get top k matches\n",
    "        values, indices = img_text_sim.topk(k)\n",
    "        \n",
    "        # Display image\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(img_pil)\n",
    "        plt.title(\"Query Image\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Display retrieved captions\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.axis('off')\n",
    "        plt.text(0, 0.5, \"Top Matching Captions:\", fontsize=14, fontweight='bold')\n",
    "        \n",
    "        for i, (score, text_idx) in enumerate(zip(values, indices)):\n",
    "            caption = tokenizer.decode(input_ids[text_idx], skip_special_tokens=True)\n",
    "            plt.text(0, 0.4 - i*0.1, f\"{i+1}. {caption[:50]}... (Score: {score:.4f})\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Demo 2: Text to Image Retrieval\n",
    "    print(\"\\n===== TEXT TO IMAGE RETRIEVAL =====\")\n",
    "    for idx in text_indices:\n",
    "        # Get a caption\n",
    "        caption = tokenizer.decode(input_ids[idx], skip_special_tokens=True)\n",
    "        \n",
    "        # Get similarity scores for this text with all images\n",
    "        text_img_sim = similarity[:, idx]\n",
    "        \n",
    "        # Get top k matches\n",
    "        values, indices = text_img_sim.topk(k)\n",
    "        \n",
    "        # Display caption and retrieved images\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        plt.suptitle(f'Query: \"{caption}\"', fontsize=16)\n",
    "        \n",
    "        for i, (score, img_idx) in enumerate(zip(values, indices)):\n",
    "            img = images[img_idx].cpu()\n",
    "            img_pil = ToPILImage()(img)\n",
    "            \n",
    "            plt.subplot(2, 3, i+1)\n",
    "            plt.imshow(img_pil)\n",
    "            plt.title(f\"Score: {score:.4f}\")\n",
    "            plt.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.subplots_adjust(top=0.9)\n",
    "        plt.show()\n",
    "demo_retrieval_with_test_loader(model, test_loader, tokenizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34c497c-6c98-4fcd-a939-0a178cf2cffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Okay so we were showing the tranformed image to 224x224 resolution. Show the original image\n",
    "def demo_retrieval_with_test_loader(model, test_loader, test_dataset, tokenizer, device, k=5):\n",
    "    \"\"\"\n",
    "    Demonstrate retrieval using samples from the test loader.\n",
    "    Displays original high-quality images instead of transformed ones.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained dual encoder model\n",
    "        test_loader: DataLoader for test data\n",
    "        test_dataset: Test dataset with access to original image paths\n",
    "        tokenizer: BERT tokenizer\n",
    "        device: Device to run model on\n",
    "        k: Number of top matches to show\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Get a batch from the test loader and keep track of indices\n",
    "    batch_idx = 0\n",
    "    for images, captions in test_loader:\n",
    "        # Calculate the absolute indices in the dataset for this batch\n",
    "        batch_size = images.shape[0]\n",
    "        dataset_indices = list(range(batch_idx * batch_size, (batch_idx * batch_size) + batch_size))\n",
    "        \n",
    "        # Move to device\n",
    "        images = images.to(device)\n",
    "        \n",
    "        # Check what kind of caption format we have\n",
    "        if isinstance(captions, torch.Tensor):\n",
    "            # If captions is just token IDs without attention mask\n",
    "            input_ids = captions.to(device)\n",
    "            attention_mask = torch.ones_like(input_ids).to(device)\n",
    "        else:\n",
    "            # If your loader returns something else, adjust accordingly\n",
    "            print(f\"Unexpected caption format: {type(captions)}\")\n",
    "            continue\n",
    "        \n",
    "        # Get all features\n",
    "        with torch.no_grad():\n",
    "            image_features, text_features = model(images, input_ids, attention_mask)\n",
    "        \n",
    "        # Convert features to CPU for processing\n",
    "        image_features = image_features.cpu()\n",
    "        text_features = text_features.cpu()\n",
    "        \n",
    "        break  # We only need one batch\n",
    "    \n",
    "    # Compute full similarity matrix\n",
    "    similarity = torch.matmul(image_features, text_features.T)\n",
    "    \n",
    "    # Get a few random indices for demo\n",
    "    num_samples = min(5, len(images))\n",
    "    sample_indices = random.sample(range(len(images)), num_samples)\n",
    "    \n",
    "    # Demo 1: Image to Text Retrieval\n",
    "    print(\"\\n===== IMAGE TO TEXT RETRIEVAL =====\")\n",
    "    for i, idx in enumerate(sample_indices):\n",
    "        # Get the dataset index for this image\n",
    "        dataset_idx = dataset_indices[idx]\n",
    "        \n",
    "        # Get original image path (adjust based on your dataset structure)\n",
    "        # You may need to modify this based on how your dataset is structured\n",
    "        image_path = test_dataset.image_paths[dataset_idx]\n",
    "        \n",
    "        # Load original high-quality image\n",
    "        try:\n",
    "            original_img = Image.open(image_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading original image {image_path}: {e}\")\n",
    "            # Fall back to transformed image if original can't be loaded\n",
    "            img = images[idx].cpu()\n",
    "            original_img = transforms.ToPILImage()(img)\n",
    "        \n",
    "        # Get similarity scores for this image with all texts\n",
    "        img_text_sim = similarity[idx]\n",
    "        \n",
    "        # Get top k matches\n",
    "        values, indices = img_text_sim.topk(k)\n",
    "        \n",
    "        # Display image\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(original_img)\n",
    "        plt.title(f\"Query Image {i+1}\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Display retrieved captions\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.axis('off')\n",
    "        plt.text(0, 0.5, \"Top Matching Captions:\", fontsize=14, fontweight='bold')\n",
    "        \n",
    "        for j, (score, text_idx) in enumerate(zip(values, indices)):\n",
    "            caption = tokenizer.decode(input_ids[text_idx], skip_special_tokens=True)\n",
    "            plt.text(0, 0.4 - j*0.08, f\"{j+1}. {caption[:80]}... (Score: {score:.4f})\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Demo 2: Text to Image Retrieval\n",
    "    print(\"\\n===== TEXT TO IMAGE RETRIEVAL =====\")\n",
    "    for i, idx in enumerate(sample_indices):\n",
    "        # Get a caption\n",
    "        caption = tokenizer.decode(input_ids[idx], skip_special_tokens=True)\n",
    "        \n",
    "        # Get similarity scores for this text with all images\n",
    "        text_img_sim = similarity[:, idx]\n",
    "        \n",
    "        # Get top k matches\n",
    "        values, indices = text_img_sim.topk(k)\n",
    "        \n",
    "        # Display caption and retrieved images\n",
    "        plt.figure(figsize=(16, 10))\n",
    "        plt.suptitle(f'Query: \"{caption}\"', fontsize=16)\n",
    "        \n",
    "        for j, (score, img_idx) in enumerate(zip(values, indices)):\n",
    "            # Get the dataset index for this image\n",
    "            dataset_img_idx = dataset_indices[img_idx]\n",
    "            \n",
    "            # Get original image path\n",
    "            image_path = test_dataset.image_paths[dataset_img_idx]\n",
    "            \n",
    "            # Load original high-quality image\n",
    "            try:\n",
    "                original_img = Image.open(image_path).convert('RGB')\n",
    "                plt.subplot(2, 3, j+1)\n",
    "                plt.imshow(original_img)\n",
    "                plt.title(f\"Score: {score:.4f}\")\n",
    "                plt.axis('off')\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading original image {image_path}: {e}\")\n",
    "                # Fall back to transformed image\n",
    "                img = images[img_idx].cpu()\n",
    "                plt.subplot(2, 3, j+1)\n",
    "                plt.imshow(transforms.ToPILImage()(img))\n",
    "                plt.title(f\"Score: {score:.4f} (low-res fallback)\")\n",
    "                plt.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.subplots_adjust(top=0.9)\n",
    "        plt.show()\n",
    "\n",
    "demo_retrieval_with_test_loader(model, test_loader, test_dataset, tokenizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98823bcf-8e9e-4bf6-aaf3-cecdba5b0746",
   "metadata": {},
   "outputs": [],
   "source": [
    "## In the above cell we can see the model performance is pretty realiable. The model can retrieve caption for an image \n",
    "## that capture the context with reasonable R score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00f9de6-6d65-4eae-a5d7-f3643d52db6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Details of the scoring metrices.\n",
    "\n",
    "\n",
    "# Recall at K (R@K): The percentage of queries where the correct match appears in the top K results.\n",
    "\n",
    "# R@1: Percentage of queries where the correct match is the top result\n",
    "# R@5: Percentage of queries where the correct match is in the top 5 results\n",
    "# R@10: Percentage of queries where the correct match is in the top 10 results\n",
    "\n",
    "\n",
    "# Median Rank: The median position of the correct match in the ranked results list.\n",
    "\n",
    "# Lower is better (1 would be perfect)\n",
    "# Less sensitive to outliers than mean rank\n",
    "\n",
    "\n",
    "# Interpretation:\n",
    "\n",
    "# Higher R@K values mean better retrieval performance\n",
    "# R@1 is the most stringent metric (perfect R@1 = 100% means the system always puts the correct match first)\n",
    "# R@5 and R@10 give credit if the correct match is among the top few results\n",
    "\n",
    "\n",
    "# Directionality:\n",
    "\n",
    "# Image-to-Text: Given an image, find the correct caption\n",
    "# Text-to-Image: Given a caption, find the correct image\n",
    "# Both directions are evaluated separately, as performance can differ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de3ad9e-812d-4b2a-a2fb-f233d1ce5cc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba74ed9c-833d-4163-b2fe-29fd792e4ad5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c4e36b04-6dcd-47f0-9251-0cd312c022ca",
   "metadata": {},
   "source": [
    "### 2. Visual Question Answering (VQA): We will use VQA dataset to train a model that can answer questions related to an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0b9466-c974-4c84-b25f-c06156a1c5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import json\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from transformers import BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1ecdae-2831-4cfc-b1c7-327a2e188951",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8408bd-0b31-47c3-8393-643a68ae4f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see what the questions and annonations data looks like\n",
    "with open('vqa_dataset/v2_OpenEnded_mscoco_train2014_questions.json', 'r') as f:\n",
    "    questions_data = json.load(f)\n",
    "with open('vqa_dataset/v2_mscoco_train2014_annotations.json', 'r') as f:\n",
    "    annotations_data = json.load(f)\n",
    "print(questions_data.keys())\n",
    "print(annotations_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b34829-5006-47e3-90ea-4cc2d49a4325",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(questions_data['questions']))\n",
    "print(len(annotations_data['annotations']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039a3259-ba3f-4d00-a3b4-821efe2ee419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check the first question and annotation\n",
    "questions_data['questions'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a07994-7120-466e-b5bd-adacdf5ac389",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_data['annotations'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6a0c44-dd42-4fad-ae74-95704d2c98c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset is quite interesting and has a lot of information. For one question (ID: 458752000), there are 10 entries for \n",
    "# for answers along with question type, answer type, confidence of answer etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1dcda6-206d-40b2-9566-850146858e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Let's understand the data. Plot answer, questions distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02eeefca-a42b-4448-85a8-6c29edd6a563",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "# Load the annotations file which contains all answers\n",
    "with open('vqa_dataset/v2_mscoco_train2014_annotations.json', 'r') as f:\n",
    "    annotations_data = json.load(f)\n",
    "\n",
    "# Extract all answers\n",
    "all_answers = [ans['answer'].lower() for ann in annotations_data['annotations'] \n",
    "               for ans in ann['answers']]\n",
    "\n",
    "# Count unique answers and their frequencies\n",
    "answer_counts = Counter(all_answers)\n",
    "unique_answers = len(answer_counts)\n",
    "\n",
    "# Print statistics\n",
    "print(f\"Total answers: {len(all_answers)}\")\n",
    "print(f\"Total unique answers: {unique_answers}\")\n",
    "\n",
    "# Calculate coverage statistics\n",
    "total_count = len(all_answers)\n",
    "for k in [10, 100, 1000, 3000, 5000]:\n",
    "    if k > unique_answers:\n",
    "        break\n",
    "    top_k_count = sum(count for _, count in answer_counts.most_common(k))\n",
    "    print(f\"Top {k} answers cover {top_k_count / total_count * 100:.2f}% of all answers\")\n",
    "\n",
    "\n",
    "# Create a better sorted bar plot\n",
    "plt.figure(figsize=(15, 6))\n",
    "counts = [count for _, count in answer_counts.most_common()]\n",
    "\n",
    "\n",
    "# Also create a log-scale version to better see the distribution\n",
    "plt.figure(figsize=(15, 6))\n",
    "tick_positions = list(range(0,5000,1000))\n",
    "plt.bar(range(5000), counts[:5000], width=1.0)\n",
    "plt.ylabel('Frequency (log scale)', fontsize=12)\n",
    "plt.title('Distribution of Top 500 Answer Frequencies (log scale)', fontsize=14)\n",
    "plt.yscale('log')\n",
    "plt.xticks(tick_positions, tick_positions)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('sorted_answer_frequencies_log.png')\n",
    "plt.show()\n",
    "\n",
    "# Create a zoomed-in view of the top 50 answers with their text labels\n",
    "plt.figure(figsize=(15, 8))\n",
    "top_50_answers = answer_counts.most_common(50)\n",
    "labels = [ans for ans, _ in top_50_answers]\n",
    "values = [count for _, count in top_50_answers]\n",
    "\n",
    "plt.bar(range(len(values)), values)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.title('Frequency of Top 50 Answers', fontsize=14)\n",
    "plt.xticks(range(len(labels)), labels, rotation=90)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('top_50_answers.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838aa6bb-9e83-4a19-8690-4c7a04f53eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "#import matplotlib.pyplot as plt\n",
    "#import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Load the training questions JSON file\n",
    "with open('vqa_dataset/v2_OpenEnded_mscoco_train2014_questions.json', 'r') as f:\n",
    "    train_questions = json.load(f)\n",
    "\n",
    "# Count questions per image\n",
    "image_question_counts = Counter()\n",
    "for question in train_questions['questions']:\n",
    "    image_id = question['image_id']\n",
    "    image_question_counts[image_id] += 1\n",
    "\n",
    "# Get statistics\n",
    "counts = list(image_question_counts.values())\n",
    "\n",
    "# Print some statistics\n",
    "print(f\"Total questions: {len(train_questions['questions'])}\")\n",
    "print(f\"Total unique images: {len(image_question_counts)}\")\n",
    "print(f\"Min questions per image: {min(counts)}\")\n",
    "print(f\"Max questions per image: {max(counts)}\")\n",
    "print(f\"Mean questions per image: {np.mean(counts):.2f}\")\n",
    "print(f\"Median questions per image: {np.median(counts)}\")\n",
    "\n",
    "# Create histogram\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(counts, bins=range(1, max(counts) + 2), alpha=0.7, edgecolor='black')\n",
    "plt.title('Number of Questions per Image in VQA Training Set', fontsize=14)\n",
    "plt.xlabel('Questions per Image', fontsize=12)\n",
    "plt.ylabel('Number of Images', fontsize=12)\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "# Add summary statistics as text\n",
    "stats_text = f\"Mean: {np.mean(counts):.2f}\\nMedian: {np.median(counts)}\\nMax: {max(counts)}\"\n",
    "plt.annotate(stats_text, xy=(0.75, 0.8), xycoords='axes fraction', \n",
    "            bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Optional: Print detailed distribution\n",
    "print(\"\\nDetailed distribution of questions per image:\")\n",
    "question_dist = Counter(counts)\n",
    "for num_q, num_images in sorted(question_dist.items()):\n",
    "    percentage = (num_images / len(image_question_counts)) * 100\n",
    "    if percentage > 1.0:  # Only show categories with >1% of images\n",
    "        print(f\"{num_q} questions: {num_images} images ({percentage:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8590d47-f3c5-4b18-80c7-d45e46cc37fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Load the annotations file which contains the answers\n",
    "with open('vqa_dataset/v2_mscoco_train2014_annotations.json', 'r') as f:\n",
    "    annotations = json.load(f)\n",
    "\n",
    "# Count unique answers per question\n",
    "unique_answers_per_question = []\n",
    "\n",
    "for ann in annotations['annotations']:\n",
    "    # Extract all answers for this question\n",
    "    answers = [a['answer'].lower().strip() for a in ann['answers']]\n",
    "    \n",
    "    # Count unique answers (case insensitive)\n",
    "    unique_count = len(set(answers))\n",
    "    unique_answers_per_question.append(unique_count)\n",
    "\n",
    "# Calculate statistics\n",
    "mean_unique = np.mean(unique_answers_per_question)\n",
    "median_unique = np.median(unique_answers_per_question)\n",
    "max_unique = np.max(unique_answers_per_question)\n",
    "\n",
    "# Create histogram\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(unique_answers_per_question, bins=range(1, max_unique + 2), alpha=0.7, edgecolor='black')\n",
    "plt.title('Number of Unique Answers per Question in VQA Training Set', fontsize=14)\n",
    "plt.xlabel('Unique Answers per Question', fontsize=12)\n",
    "plt.ylabel('Number of Questions', fontsize=12)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.xticks(range(1, max_unique + 1))\n",
    "\n",
    "# Add summary statistics as text\n",
    "stats_text = f\"Mean: {mean_unique:.2f}\\nMedian: {median_unique:.1f}\\nMax: {max_unique}\"\n",
    "plt.annotate(stats_text, xy=(0.75, 0.8), xycoords='axes fraction', \n",
    "            bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed distribution\n",
    "print(\"\\nDistribution of unique answers per question:\")\n",
    "answer_dist = Counter(unique_answers_per_question)\n",
    "for num_a, num_questions in sorted(answer_dist.items()):\n",
    "    percentage = (num_questions / len(unique_answers_per_question)) * 100\n",
    "    print(f\"{num_a} unique answers: {num_questions} questions ({percentage:.2f}%)\")\n",
    "\n",
    "# Additionally, you can analyze agreement among annotators\n",
    "print(\"\\nAnalyzing annotator agreement:\")\n",
    "agreement_scores = []\n",
    "\n",
    "for ann in annotations['annotations']:\n",
    "    answers = [a['answer'].lower().strip() for a in ann['answers']]\n",
    "    answer_counts = Counter(answers)\n",
    "    \n",
    "    # Calculate the percentage of annotators who gave the most common answer\n",
    "    most_common_answer_count = answer_counts.most_common(1)[0][1]\n",
    "    agreement_score = most_common_answer_count / len(ann['answers'])\n",
    "    agreement_scores.append(agreement_score)\n",
    "\n",
    "# Plot agreement distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(agreement_scores, bins=np.linspace(0, 1, 11), alpha=0.7, edgecolor='black')\n",
    "plt.title('Annotator Agreement on Answers in VQA Training Set', fontsize=14)\n",
    "plt.xlabel('Fraction of Annotators Giving Most Common Answer', fontsize=12)\n",
    "plt.ylabel('Number of Questions', fontsize=12)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.xticks(np.linspace(0, 1, 11))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print agreement distribution\n",
    "print(\"\\nDistribution of annotator agreement:\")\n",
    "agreement_ranges = [(0.1*i, 0.1*(i+1)) for i in range(10)]\n",
    "for lower, upper in agreement_ranges:\n",
    "    count = sum(1 for score in agreement_scores if lower <= score < upper)\n",
    "    if upper == 1.0:  # Include 1.0 in the last bin\n",
    "        count = sum(1 for score in agreement_scores if lower <= score <= upper)\n",
    "    percentage = (count / len(agreement_scores)) * 100\n",
    "    print(f\"{lower:.1f}-{upper:.1f}: {count} questions ({percentage:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9669babc-1d7f-4920-ace8-3271a7e24815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's build a dataset: Build a vocabulary set with top 3000 most frequent answers (as that covers around 90% of the total answers)\n",
    "#and leave other as unknown. Transform the  image and tokenize the questions. Make \"1 image-1 questions\" a datapoint along with \n",
    "#a answer weight distribution. So total datapoint would be same as number of questions. Also we can see that there are about \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588d9337-5aed-4da2-aac4-bcb2ae0e4979",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQADataset(Dataset):\n",
    "    def __init__(self, questions_file, annotations_file, image_dir, transform=None, tokenizer=None, max_length=32, answer_top_k=3000):\n",
    "        # Load questions\n",
    "        with open(questions_file, 'r') as f:\n",
    "            self.questions = json.load(f)['questions']\n",
    "            \n",
    "        # Load annotations if available (not for test set)\n",
    "        self.has_annotations = annotations_file is not None\n",
    "        if self.has_annotations:\n",
    "            with open(annotations_file, 'r') as f:\n",
    "                self.annotations = json.load(f)['annotations']\n",
    "            \n",
    "            # Create answer vocabulary (top k most frequent answers)\n",
    "            all_answers = [ans['answer'] for ann in self.annotations for ans in ann['answers']]\n",
    "            answer_counts = pd.Series(all_answers).value_counts()\n",
    "            self.answer_vocab = {ans: idx for idx, ans in enumerate(answer_counts.index[:answer_top_k])}\n",
    "            self.unk_ans_idx = len(self.answer_vocab)\n",
    "        \n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform or transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        self.tokenizer = tokenizer or AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.questions)\n",
    "    \n",
    "    # def __getitem__(self, idx):\n",
    "    #     # Get question and image info\n",
    "    #     question_data = self.questions[idx]\n",
    "    #     question_id = question_data['question_id']\n",
    "    #     image_id = question_data['image_id']\n",
    "    #     question_text = question_data['question']\n",
    "        \n",
    "        # Load and transform image\n",
    "        #image_filename = f\"COCO_train2014_{image_id:012d}.jpg\"  # COCO format\n",
    "        #image_path = os.path.join(self.image_dir, image_filename)\n",
    "        #image = Image.open(image_path).convert('RGB')\n",
    "    def __getitem__(self, idx):\n",
    "        # Get question and image info\n",
    "        question_data = self.questions[idx]\n",
    "        question_id = question_data['question_id']\n",
    "        image_id = question_data['image_id']\n",
    "        question_text = question_data['question']\n",
    "        \n",
    "        # Determine the correct image filename format based on the directory\n",
    "        if 'train' in self.image_dir:\n",
    "            image_filename = f\"COCO_train2014_{image_id:012d}.jpg\"\n",
    "        elif 'val' in self.image_dir:\n",
    "            image_filename = f\"COCO_val2014_{image_id:012d}.jpg\"\n",
    "        else:\n",
    "            # For test or other directories\n",
    "            image_filename = f\"COCO_test2015_{image_id:012d}.jpg\"  # Adjust as needed\n",
    "        \n",
    "        image_path = os.path.join(self.image_dir, image_filename)\n",
    "        \n",
    "        # Add error handling for missing images\n",
    "        try:\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: Image not found at {image_path}. Trying alternative format...\")\n",
    "            # Try alternative format as fallback\n",
    "            image_filename = f\"{image_id:012d}.jpg\"\n",
    "            image_path = os.path.join(self.image_dir, image_filename)\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "        \n",
    "        \n",
    "        \n",
    "        image = self.transform(image)\n",
    "        \n",
    "        # Tokenize question\n",
    "        question_tokens = self.tokenizer(\n",
    "            question_text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Extract tokens and attention mask\n",
    "        input_ids = question_tokens['input_ids'].squeeze(0)\n",
    "        attention_mask = question_tokens['attention_mask'].squeeze(0)\n",
    "        \n",
    "        # If we have annotations, prepare answer target\n",
    "        if self.has_annotations:\n",
    "            # Find corresponding annotation\n",
    "            annotation = next(ann for ann in self.annotations if ann['question_id'] == question_id)\n",
    "            \n",
    "            # Create answer target (multi-hot encoding)\n",
    "            answers = [ans['answer'] for ans in annotation['answers']]\n",
    "            answer_weights = torch.zeros(len(self.answer_vocab) + 1)  # +1 for unknown\n",
    "            \n",
    "            for answer in answers:\n",
    "                if answer in self.answer_vocab:\n",
    "                    ans_idx = self.answer_vocab[answer]\n",
    "                    # Normalize by number of annotators (usually 10)\n",
    "                    answer_weights[ans_idx] += 1.0 / len(answers)\n",
    "                else:\n",
    "                    # Unknown answer\n",
    "                    answer_weights[self.unk_ans_idx] += 1.0 / len(answers)\n",
    "                    \n",
    "            return {\n",
    "                'image': image,\n",
    "                'input_ids': input_ids,\n",
    "                'attention_mask': attention_mask,\n",
    "                'answer_weights': answer_weights,\n",
    "                'question_id': question_id\n",
    "            }\n",
    "        else:\n",
    "            # For test set, we don't have annotations\n",
    "            return {\n",
    "                'image': image,\n",
    "                'input_ids': input_ids,\n",
    "                'attention_mask': attention_mask,\n",
    "                'question_id': question_id\n",
    "            }\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8f0a52-9a62-4556-8ecd-954e6f87f8dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25089ac-93bd-4d7e-99e6-8f9c22baa138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data setup\n",
    "train_dataset = VQADataset(\n",
    "    questions_file='vqa_dataset/v2_OpenEnded_mscoco_train2014_questions.json',\n",
    "    annotations_file='vqa_dataset/v2_mscoco_train2014_annotations.json',\n",
    "    image_dir='vqa_dataset/train2014'\n",
    ")\n",
    "\n",
    "val_dataset = VQADataset(\n",
    "    questions_file='vqa_dataset/v2_OpenEnded_mscoco_val2014_questions.json',\n",
    "    annotations_file='vqa_dataset/v2_mscoco_val2014_annotations.json',\n",
    "    image_dir='vqa_dataset/val2014',\n",
    "    answer_top_k=train_dataset.unk_ans_idx  # Use same vocabulary as training\n",
    ")\n",
    "\n",
    "# Load test dataset (without annotations)\n",
    "test_dataset = VQADataset(\n",
    "    questions_file='vqa_dataset/v2_OpenEnded_mscoco_test2015_questions.json',\n",
    "    annotations_file=None,  # No annotations for test set\n",
    "    image_dir='vqa_dataset/test2015'\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# Print number of batches and samples in the train_loader\n",
    "num_batches = len(train_loader)\n",
    "num_samples = len(train_dataset)\n",
    "batch_size = train_loader.batch_size\n",
    "\n",
    "print(f\"Training DataLoader:\")\n",
    "print(f\"Total samples: {num_samples}\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Number of batches: {num_batches}\")\n",
    "\n",
    "# To get a sample batch and print its dimensions\n",
    "sample_batch = next(iter(train_loader))\n",
    "print(\"\\nSample batch dimensions:\")\n",
    "for key, value in sample_batch.items():\n",
    "    if isinstance(value, torch.Tensor):\n",
    "        print(f\"{key}: {value.shape}\")\n",
    "    else:\n",
    "        print(f\"{key}: {type(value)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d993a112-c5de-4e5c-9f5e-fc5aa1223a6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c61138c-c692-488a-84b3-cdc2b1987170",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca9f2b3-1a47-4c10-8958-3c5461b99d85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d830601-cee6-49bb-ba26-64bd56291e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's see what we have in train_loader\n",
    "# Get a batch from the train_loader\n",
    "for batch in train_loader:\n",
    "    # Extract the first example from the batch\n",
    "    image = batch['image'][0]\n",
    "    input_ids = batch['input_ids'][0]\n",
    "    answer_weights = batch['answer_weights'][0] if 'answer_weights' in batch else None\n",
    "    \n",
    "    # Decode the question from input_ids\n",
    "    question_text = tokenizer.decode(input_ids, skip_special_tokens=True)\n",
    "    \n",
    "    # Find top answers if answer_weights are available\n",
    "    top_answers = []\n",
    "    if answer_weights is not None:\n",
    "        # Get indices of top 3 answers with highest weights\n",
    "        top_indices = torch.topk(answer_weights, k=min(3, len(answer_weights))).indices\n",
    "        for idx in top_indices:\n",
    "            if idx in idx_to_answer:\n",
    "                weight = answer_weights[idx].item()\n",
    "                top_answers.append((idx_to_answer[idx], weight))\n",
    "    \n",
    "    # Display the image and information\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    # Convert tensor to image for display\n",
    "    img = image.permute(1, 2, 0).cpu().numpy()\n",
    "    # Denormalize the image\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    img = std * img + mean\n",
    "    img = np.clip(img, 0, 1)\n",
    "    \n",
    "    plt.imshow(img)\n",
    "    plt.title(f\"Question: {question_text}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Question: {question_text}\")\n",
    "    \n",
    "    if top_answers:\n",
    "        print(\"\\nTop Answers:\")\n",
    "        for answer, weight in top_answers:\n",
    "            print(f\"- {answer} (weight: {weight:.4f})\")\n",
    "    \n",
    "    # Only show one example and then break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a41e9a-5dfd-4d63-8aa8-98de0e64ed67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some of the images: Function to denormalize image for visualization\n",
    "def denormalize(tensor):\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "    return tensor * std + mean\n",
    "    \n",
    "def show_multiple_questions_efficiently(dataset, num_images=3, questions_per_image=5):\n",
    "    # Create index lookup first (do this only once)\n",
    "    if not hasattr(dataset, 'image_id_to_indices'):\n",
    "        print(\"Building image_id index (first time only)...\")\n",
    "        dataset.image_id_to_indices = {}\n",
    "        dataset.question_id_to_answers = {}\n",
    "        \n",
    "        # Pre-index questions by image_id\n",
    "        for i, question_data in enumerate(dataset.questions):\n",
    "            image_id = question_data['image_id']\n",
    "            question_id = question_data['question_id']\n",
    "            \n",
    "            if image_id not in dataset.image_id_to_indices:\n",
    "                dataset.image_id_to_indices[image_id] = []\n",
    "            dataset.image_id_to_indices[image_id].append(i)\n",
    "            \n",
    "        # Pre-index answers by question_id if annotations exist\n",
    "        if dataset.has_annotations:\n",
    "            for ann in dataset.annotations:\n",
    "                question_id = ann['question_id']\n",
    "                answers = [a['answer'] for a in ann['answers']]\n",
    "                most_common = max(set(answers), key=answers.count)\n",
    "                dataset.question_id_to_answers[question_id] = most_common\n",
    "    \n",
    "    # Find images with enough questions\n",
    "    image_ids_with_multiple = [\n",
    "        img_id for img_id, indices in dataset.image_id_to_indices.items() \n",
    "        if len(indices) >= questions_per_image\n",
    "    ]\n",
    "    \n",
    "    # Select random images\n",
    "    selected_image_ids = random.sample(image_ids_with_multiple, min(num_images, len(image_ids_with_multiple)))\n",
    "    \n",
    "    # Display each image with its questions\n",
    "    for img_id in selected_image_ids:\n",
    "        # Get the first dataset index for this image\n",
    "        sample_idx = dataset.image_id_to_indices[img_id][0]\n",
    "        sample = dataset[sample_idx]\n",
    "        \n",
    "        # Get and denormalize image\n",
    "        img = denormalize(sample['image'])\n",
    "        img = img.permute(1, 2, 0).numpy()\n",
    "        img = np.clip(img, 0, 1)\n",
    "        \n",
    "        # Get questions for this image\n",
    "        question_indices = dataset.image_id_to_indices[img_id][:questions_per_image]\n",
    "        questions_data = []\n",
    "        \n",
    "        for idx in question_indices:\n",
    "            q_data = dataset.questions[idx]\n",
    "            question_text = q_data['question']\n",
    "            question_id = q_data['question_id']\n",
    "            \n",
    "            # Get answer if available\n",
    "            answer = \"N/A\"\n",
    "            if dataset.has_annotations and question_id in dataset.question_id_to_answers:\n",
    "                answer = dataset.question_id_to_answers[question_id]\n",
    "                \n",
    "            questions_data.append({\n",
    "                'question_text': question_text,\n",
    "                'answer': answer\n",
    "            })\n",
    "        \n",
    "        # Display image with questions\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.imshow(img)\n",
    "        \n",
    "        title = f\"Image ID: {img_id}\\n\"\n",
    "        for i, q_data in enumerate(questions_data):\n",
    "            title += f\"Q{i+1}: {q_data['question_text']}\\nA{i+1}: {q_data['answer']}\\n\"\n",
    "            \n",
    "        plt.title(title)\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        print(\"\\n\")\n",
    "\n",
    "# Use the more efficient function\n",
    "show_multiple_questions_efficiently(train_dataset, num_images=3, questions_per_image=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85523a47-6ae2-41ef-b89b-86f801869ce5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcb1679-917e-4e06-8921-123eddf9a601",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Show the image with maximum number of questions\n",
    "\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Load the questions and annotations files\n",
    "with open('vqa_dataset/v2_OpenEnded_mscoco_train2014_questions.json', 'r') as f:\n",
    "    questions_data = json.load(f)\n",
    "\n",
    "with open('vqa_dataset/v2_mscoco_train2014_annotations.json', 'r') as f:\n",
    "    annotations_data = json.load(f)\n",
    "\n",
    "# Count questions per image\n",
    "image_question_counts = Counter()\n",
    "for question in questions_data['questions']:\n",
    "    image_id = question['image_id']\n",
    "    image_question_counts[image_id] += 1\n",
    "\n",
    "# Find the image with the most questions\n",
    "max_questions_image_id = image_question_counts.most_common(1)[0][0]\n",
    "max_questions_count = image_question_counts[max_questions_image_id]\n",
    "\n",
    "print(f\"Image ID with most questions: {max_questions_image_id}\")\n",
    "print(f\"Number of questions: {max_questions_count}\")\n",
    "\n",
    "# Find all questions for this image\n",
    "image_questions = []\n",
    "for question in questions_data['questions']:\n",
    "    if question['image_id'] == max_questions_image_id:\n",
    "        # Find the corresponding annotation to get answers\n",
    "        for annotation in annotations_data['annotations']:\n",
    "            if annotation['question_id'] == question['question_id']:\n",
    "                # Get most common answer\n",
    "                answers = [a['answer'] for a in annotation['answers']]\n",
    "                most_common_answer = Counter(answers).most_common(1)[0][0]\n",
    "                \n",
    "                image_questions.append({\n",
    "                    'question_id': question['question_id'],\n",
    "                    'question': question['question'],\n",
    "                    'most_common_answer': most_common_answer,\n",
    "                    'all_answers': answers\n",
    "                })\n",
    "                break\n",
    "\n",
    "# Load and display the image (adapt the path to your COCO images location)\n",
    "image_filename = f\"COCO_train2014_{max_questions_image_id:012d}.jpg\"  # COCO 2014 format\n",
    "image_path = f\"vqa_dataset/train2014/{image_filename}\"  # Adjust path as needed\n",
    "\n",
    "try:\n",
    "    img = Image.open(image_path)\n",
    "    \n",
    "    # Display image\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(np.asarray(img))\n",
    "    plt.title(f\"Image with Most Questions: {max_questions_count} questions\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    # Print all questions and answers\n",
    "    print(\"\\nQuestions and Answers for this image:\")\n",
    "    for i, q in enumerate(image_questions):\n",
    "        print(f\"{i+1}. Q: {q['question']}\")\n",
    "        print(f\"   A: {q['most_common_answer']} (Most common of {len(set(q['all_answers']))} unique answers)\")\n",
    "        \n",
    "except FileNotFoundError:\n",
    "    print(f\"Image file not found at {image_path}. Please adjust the path to your COCO images.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d08279-e986-4426-9a83-db37ef3504c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a model now: ResNet to encode image. Bert to encode the questions. Use cross attention on image and text. Predict answer\n",
    "# weight distribution and estimate how similar it is with the real distribution using BCEWithLogitsLoss loss function.\n",
    "\n",
    "class CrossAttentionFusion(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(CrossAttentionFusion, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(hidden_size, num_heads=8)\n",
    "        \n",
    "    def forward(self, image_features, question_features):\n",
    "        # Reshape for attention (seq_len, batch, features)\n",
    "        image_features = image_features.unsqueeze(0)  # [1, batch, hidden]\n",
    "        question_features = question_features.unsqueeze(0)  # [1, batch, hidden]\n",
    "        \n",
    "        # Cross-attention: question attends to image\n",
    "        attn_output, _ = self.attention(\n",
    "            query=question_features,\n",
    "            key=image_features,\n",
    "            value=image_features\n",
    "        )\n",
    "        \n",
    "        # Return attended features\n",
    "        return attn_output.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f149c2e-2da2-48d6-9316-4c0ee5ca1fb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40b9025-21ea-470e-bd24-f0019e64efaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQAModel(nn.Module):\n",
    "    def __init__(self, num_answers, hidden_size=512):\n",
    "        super(VQAModel, self).__init__()\n",
    "        \n",
    "        # Image Encoder (ResNet)\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        modules = list(resnet.children())[:-1]  # Remove final FC layer\n",
    "        self.image_encoder = nn.Sequential(*modules)\n",
    "        \n",
    "        # Question Encoder (BERT)\n",
    "        self.question_encoder = BertModel.from_pretrained('bert-base-uncased')\n",
    "        \n",
    "        # Projection layers\n",
    "        self.image_projection = nn.Linear(2048, hidden_size)\n",
    "        self.question_projection = nn.Linear(768, hidden_size)\n",
    "        \n",
    "        # Cross-attention fusion\n",
    "        self.cross_attention = CrossAttentionFusion(hidden_size)\n",
    "        \n",
    "        # Classification\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(hidden_size, num_answers)\n",
    "        )\n",
    "        \n",
    "    def forward(self, image, input_ids, attention_mask):\n",
    "        # Image encoding\n",
    "        batch_size = image.size(0)\n",
    "        image_features = self.image_encoder(image).squeeze(-1).squeeze(-1)\n",
    "        image_features = self.image_projection(image_features)\n",
    "        \n",
    "        # Question encoding\n",
    "        question_outputs = self.question_encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        question_features = question_outputs.pooler_output\n",
    "        question_features = self.question_projection(question_features)\n",
    "        \n",
    "        # Cross-attention fusion\n",
    "        fused_features = self.cross_attention(image_features, question_features)\n",
    "        \n",
    "        # Classification\n",
    "        output = self.classifier(fused_features)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ceba32e-e58a-4a20-9273-2249e99db1da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb10b0e4-7a6f-40ae-bc18-5ded7e870cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = VQAModel(num_answers=len(train_dataset.answer_vocab) + 1)  # +1 for unknown answers\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, factor=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70c3c5b-ab41-44c5-8a1c-d00ae2ab5946",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08bbb77-ee4b-4981-8197-fced8d681686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop with checkpointing\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=10):\n",
    "    best_val_accuracy = 0.0\n",
    "    t_loss=[]\n",
    "    v_loss=[]\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\"):\n",
    "            # Get data\n",
    "            images = batch['image'].to(device)\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            answer_weights = batch['answer_weights'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images, input_ids, attention_mask)\n",
    "            loss = criterion(outputs, answer_weights)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        t_loss.append(avg_train_loss)\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\"):\n",
    "                # Get data\n",
    "                images = batch['image'].to(device)\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                answer_weights = batch['answer_weights'].to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(images, input_ids, attention_mask)\n",
    "                loss = criterion(outputs, answer_weights)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                # Calculate accuracy (VQA accuracy metric)\n",
    "                pred_indices = torch.argmax(outputs, dim=1)\n",
    "                ground_truth_indices = torch.argmax(answer_weights, dim=1)\n",
    "                correct += (pred_indices == ground_truth_indices).sum().item()\n",
    "                total += images.size(0)\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_accuracy = 100 * correct / total\n",
    "        v_loss.append(avg_val_loss)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"Training Loss: {avg_train_loss:.4f}\")\n",
    "        print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "        print(f\"Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step(avg_val_loss)\n",
    "        # Save model every 5 epochs\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_accuracy': val_accuracy,\n",
    "                'train_loss': avg_train_loss,\n",
    "                'val_loss': avg_val_loss,\n",
    "                'answer_vocab': train_dataset.answer_vocab\n",
    "            }, f'vqa_model_epoch_{epoch+1}.pth')\n",
    "            print(f\"Saved model checkpoint at epoch {epoch+1}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_accuracy': val_accuracy,\n",
    "                'answer_vocab': train_dataset.answer_vocab\n",
    "            }, 'best_vqa_model.pth')\n",
    "            print(f\"Saved new best model with accuracy: {val_accuracy:.2f}%\")\n",
    "    \n",
    "    return model, t_loss,v_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700c952b-c565-4efd-8afe-8f374686e56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training\n",
    "model,train_loss,valid_loss = train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a14149e-4465-4930-a554-bf65561cad75",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run few more steps as the validation accuracy is bad\n",
    "# checkpoint = torch.load('best_vqa_model.pth')\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "#model, more_train_loss, more_valid_loss = train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=5)  # Add 6 more epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fd2b0c-4213-43aa-8ce7-09cccea5f8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The accurcay looks better now. plot losses\n",
    "\n",
    "# train_loss_all=train_loss+more_train_loss # combine from 2 stages of training\n",
    "# valid_loss_all=valid_loss+more_valid_loss\n",
    "\n",
    "# plt.plot(train_loss_all, 'b', label='Training loss')\n",
    "# plt.plot(valid_loss_all, 'r', label='Validation loss')\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_loss, 'b', label='Training loss')\n",
    "plt.plot(valid_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c624334a-4a78-4411-b033-e402ef7c9a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use a standard accuracy matrix for this model. For each questions there are 10 answer. If there are >1 unique answers, what is\n",
    "## right answer to use? In training we use a probablity distribution. But for accuracy estimation the standard approach is if =>3 \n",
    "## annotators gave the same answer for a question, that's the right answer. Therefore, we use following equation\n",
    "## min(number of annotations gave that answer/3 ,1). So if 3 or >3 humans gave that answer, accuracy is 1, else, that number/3. \n",
    "## This way we get a very intuitive accuracy estimation.\n",
    "\n",
    "def vqa_accuracy(predicted, target):\n",
    "    \"\"\"\n",
    "    VQA accuracy metric: min(#humans who gave that answer / 3, 1)\n",
    "    \"\"\"\n",
    "    batch_size = predicted.size(0)\n",
    "    accuracy = 0.0\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        # Get predicted answer\n",
    "        pred_idx = torch.argmax(predicted[i]).item()\n",
    "        \n",
    "        # Check if this matches any ground truth answer\n",
    "        accuracy += min(target[i, pred_idx].item() * 3, 1.0)\n",
    "    \n",
    "    return accuracy / batch_size\n",
    "\n",
    "def evaluate_model(model, data_loader):\n",
    "    model.eval()\n",
    "    total_accuracy = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "            # Get data\n",
    "            images = batch['image'].to(device)\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            answer_weights = batch['answer_weights'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images, input_ids, attention_mask)\n",
    "            \n",
    "            # Calculate VQA accuracy\n",
    "            batch_accuracy = vqa_accuracy(outputs, answer_weights)\n",
    "            total_accuracy += batch_accuracy\n",
    "            num_batches += 1\n",
    "    \n",
    "    return total_accuracy / num_batches\n",
    "\n",
    "# Evaluate on validation set\n",
    "val_accuracy = evaluate_model(model, val_loader)\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28e4c55-1650-4bbb-9d10-1112bd3651d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Validation accuracy is too low ~0.25. This is not surprising as we trained only for 4 epochs. Let's see how it performs\n",
    "### on few of the example from the validation and test sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6660dcf3-34ea-46ba-9322-860ac7121437",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9becbe8-e9fc-4a01-a5c3-b79310a2e40b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d96528-b4d6-4609-894d-05cb0df6e119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a few examples from validation set\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Create index to answer mapping\n",
    "idx_to_answer = {idx: ans for ans, idx in val_dataset.answer_vocab.items()}\n",
    "\n",
    "val_examples = []\n",
    "for i, batch in enumerate(val_loader):\n",
    "    if i >= 5:  # Get 5 examples\n",
    "        break\n",
    "    \n",
    "    # Get question_id to find the ground truth\n",
    "    question_id = batch['question_id'][0].item()\n",
    "    \n",
    "    # Find matching question text for this question_id\n",
    "    question_text = None\n",
    "    for q in val_dataset.questions:\n",
    "        if q['question_id'] == question_id:\n",
    "            question_text = q['question']\n",
    "            break\n",
    "    \n",
    "    # If question text not found, decode from input_ids as fallback\n",
    "    if question_text is None:\n",
    "        input_ids = batch['input_ids'][0]\n",
    "        question_text = tokenizer.decode(input_ids, skip_special_tokens=True)\n",
    "    \n",
    "    # Find matching annotation for ground truth\n",
    "    ground_truth = \"Unknown\"\n",
    "    for ann in val_dataset.annotations:\n",
    "        if ann['question_id'] == question_id:\n",
    "            # Get most common answer\n",
    "            answers = [a['answer'] for a in ann['answers']]\n",
    "            from collections import Counter\n",
    "            ground_truth = Counter(answers).most_common(1)[0][0]\n",
    "            break\n",
    "    \n",
    "    val_examples.append({\n",
    "        'image': batch['image'][0],\n",
    "        'question_id': question_id,\n",
    "        'question_text': question_text,\n",
    "        'ground_truth': ground_truth\n",
    "    })\n",
    "\n",
    "# Run inference on validation examples\n",
    "for i, example in enumerate(val_examples):\n",
    "    # Get image and question\n",
    "    image = example['image'].to(device)\n",
    "    question = example['question_text']\n",
    "    ground_truth = example['ground_truth']\n",
    "    \n",
    "    # Tokenize question\n",
    "    question_tokens = tokenizer(\n",
    "        question,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=32,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    input_ids = question_tokens['input_ids'].to(device)\n",
    "    attention_mask = question_tokens['attention_mask'].to(device)\n",
    "    \n",
    "    # Get prediction\n",
    "    with torch.no_grad():\n",
    "        output = model(image.unsqueeze(0), input_ids, attention_mask)\n",
    "        predicted_idx = torch.argmax(output, dim=1).item()\n",
    "    \n",
    "    # Convert index to answer\n",
    "    if predicted_idx in idx_to_answer:\n",
    "        predicted_answer = idx_to_answer[predicted_idx]\n",
    "    else:\n",
    "        predicted_answer = \"unknown\"\n",
    "    \n",
    "    # Visualize\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(example['image'].permute(1, 2, 0).cpu())\n",
    "    plt.title(f\"Validation Example {i+1}\\nQ: {question}\\nPredicted: {predicted_answer}\\nGround Truth: {ground_truth}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Validation Example {i+1}:\")\n",
    "    print(f\"Question ID: {example['question_id']}\")\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Predicted Answer: {predicted_answer}\")\n",
    "    print(f\"Ground Truth: {ground_truth}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a45ffd-13cf-4f02-80d9-75f27c428513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a few examples from test set\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Create index to answer mapping\n",
    "idx_to_answer = {idx: ans for ans, idx in train_dataset.answer_vocab.items()}\n",
    "\n",
    "test_examples = []\n",
    "for i, batch in enumerate(test_loader):\n",
    "    if i >= 5:  # Get 5 examples\n",
    "        break\n",
    "    \n",
    "    # Get question_id\n",
    "    question_id = batch['question_id'][0].item()\n",
    "    \n",
    "    # Find matching question text for this question_id\n",
    "    question_text = None\n",
    "    for q in test_dataset.questions:\n",
    "        if q['question_id'] == question_id:\n",
    "            question_text = q['question']\n",
    "            break\n",
    "    \n",
    "    # If question text not found, decode from input_ids as fallback\n",
    "    if question_text is None and 'input_ids' in batch:\n",
    "        input_ids = batch['input_ids'][0]\n",
    "        question_text = tokenizer.decode(input_ids, skip_special_tokens=True)\n",
    "    \n",
    "    test_examples.append({\n",
    "        'image': batch['image'][0],\n",
    "        'question_id': question_id,\n",
    "        'question_text': question_text\n",
    "    })\n",
    "\n",
    "# Run inference on test examples\n",
    "for i, example in enumerate(test_examples):\n",
    "    # Get image and question\n",
    "    image = example['image'].to(device)\n",
    "    question = example['question_text']\n",
    "    \n",
    "    # Tokenize question\n",
    "    question_tokens = tokenizer(\n",
    "        question,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=32,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    input_ids = question_tokens['input_ids'].to(device)\n",
    "    attention_mask = question_tokens['attention_mask'].to(device)\n",
    "    \n",
    "    # Get prediction\n",
    "    with torch.no_grad():\n",
    "        output = model(image.unsqueeze(0), input_ids, attention_mask)\n",
    "        predicted_idx = torch.argmax(output, dim=1).item()\n",
    "    \n",
    "    # Convert index to answer\n",
    "    if predicted_idx in idx_to_answer:\n",
    "        predicted_answer = idx_to_answer[predicted_idx]\n",
    "    else:\n",
    "        predicted_answer = \"unknown\"\n",
    "    \n",
    "    # Visualize\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(example['image'].permute(1, 2, 0).cpu())\n",
    "    plt.title(f\"Test Example {i+1}\\nQ: {question}\\nPredicted: {predicted_answer}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Test Example {i+1}:\")\n",
    "    print(f\"Question ID: {example['question_id']}\")\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Predicted Answer: {predicted_answer}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79207fcb-5a80-410f-ab83-535a94db3cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Predict answer for an user provided image and questions. Can be any image and questions. How does it perform?\n",
    "\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "def predict_answer(model, image_path, question, tokenizer, answer_vocab, device):\n",
    "    # Load and preprocess image\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Handle both local paths and URLs\n",
    "    if image_path.startswith('http'):\n",
    "        response = requests.get(image_path)\n",
    "        image = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "    else:\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "    \n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Tokenize question\n",
    "    question_tokens = tokenizer(\n",
    "        question,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=32,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    input_ids = question_tokens['input_ids'].to(device)\n",
    "    attention_mask = question_tokens['attention_mask'].to(device)\n",
    "    \n",
    "    # Get prediction\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(image_tensor, input_ids, attention_mask)\n",
    "        predicted_idx = torch.argmax(output, dim=1).item()\n",
    "    \n",
    "    # Convert index to answer\n",
    "    idx_to_answer = {idx: ans for ans, idx in answer_vocab.items()}\n",
    "    if predicted_idx in idx_to_answer:\n",
    "        answer = idx_to_answer[predicted_idx]\n",
    "    else:\n",
    "        answer = \"unknown\"\n",
    "    \n",
    "    # Visualize\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(image)\n",
    "    plt.title(f\"Q: {question}\\nA: {answer}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    return answer\n",
    "\n",
    "# Load checkpoint and answer vocabulary\n",
    "checkpoint = torch.load('best_vqa_model.pth', map_location=device)\n",
    "answer_vocab = checkpoint['answer_vocab']\n",
    "\n",
    "# Load model with pretrained weights\n",
    "model = VQAModel(num_answers=len(answer_vocab) + 1)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.to(device)\n",
    "\n",
    "# Example prediction\n",
    "test_image = \"generate_test.JPG\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76802284-973e-4a50-9a20-877eecda41f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_question = \"How many dogs are there?\"\n",
    "answer = predict_answer(model, test_image, test_question, tokenizer, answer_vocab, device)\n",
    "print(f\"Predicted answer: {answer}\")\n",
    "plt.figure(figsize=(10, 10))\n",
    "#plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbd584a-2d3b-46bd-8cdf-3958666033ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_question = \"How many girls are there?\"\n",
    "answer = predict_answer(model, test_image, test_question, tokenizer, answer_vocab, device)\n",
    "print(f\"Predicted answer: {answer}\")\n",
    "plt.figure(figsize=(10, 10))\n",
    "#plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ddd9e2-d6ae-466b-a183-155ab8c7379b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_question = \"What color is the girl's top?\"\n",
    "answer = predict_answer(model, test_image, test_question, tokenizer, answer_vocab, device)\n",
    "print(f\"Predicted answer: {answer}\")\n",
    "plt.figure(figsize=(10, 10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9908c52-4ef2-4416-99a6-d5fa2a1c40b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ask an unrelevant question\n",
    "\n",
    "test_question = \"What fruit is it?\"\n",
    "answer = predict_answer(model, test_image, test_question, tokenizer, answer_vocab, device)\n",
    "print(f\"Predicted answer: {answer}\")\n",
    "plt.figure(figsize=(10, 10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b52c64-4c10-400f-b2e6-1e26da6172fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### All seems \"No\" answer. let me see the distribution from trained model\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "def analyze_prediction_distribution(model, data_loader, idx_to_answer, device, max_samples=1000):\n",
    "    \"\"\"Analyze the distribution of answers predicted by the model\"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    \n",
    "    # Process batches\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(tqdm(data_loader, desc=\"Analyzing predictions\")):\n",
    "            if i * batch['image'].size(0) >= max_samples:\n",
    "                break\n",
    "                \n",
    "            # Get data\n",
    "            images = batch['image'].to(device)\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images, input_ids, attention_mask)\n",
    "            \n",
    "            # Get predicted answers\n",
    "            pred_indices = torch.argmax(outputs, dim=1)\n",
    "            \n",
    "            # Convert to answer text\n",
    "            for idx in pred_indices:\n",
    "                idx = idx.item()\n",
    "                if idx in idx_to_answer:\n",
    "                    predictions.append(idx_to_answer[idx])\n",
    "                else:\n",
    "                    predictions.append(\"unknown\")\n",
    "    \n",
    "    # Count predictions\n",
    "    prediction_counts = Counter(predictions)\n",
    "    \n",
    "    # Plot top predictions\n",
    "    top_k = 20  # Show top 20 most frequent answers\n",
    "    top_predictions = prediction_counts.most_common(top_k)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Extract labels and values\n",
    "    labels = [item[0] for item in top_predictions]\n",
    "    values = [item[1] for item in top_predictions]\n",
    "    \n",
    "    # Create horizontal bar chart\n",
    "    y_pos = np.arange(len(labels))\n",
    "    plt.barh(y_pos, values)\n",
    "    plt.yticks(y_pos, labels)\n",
    "    plt.xlabel('Count')\n",
    "    plt.title(f'Top {top_k} Predicted Answers')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics\n",
    "    total_predictions = len(predictions)\n",
    "    unique_predictions = len(prediction_counts)\n",
    "    top_10_percent = sum(count for _, count in top_predictions[:int(unique_predictions * 0.1)])\n",
    "    \n",
    "    print(f\"Total predictions analyzed: {total_predictions}\")\n",
    "    print(f\"Unique answers predicted: {unique_predictions}\")\n",
    "    print(f\"Top 10% of answers account for {top_10_percent/total_predictions:.2%} of all predictions\")\n",
    "    \n",
    "    return prediction_counts\n",
    "\n",
    "# Run the analysis on validation set\n",
    "prediction_distribution = analyze_prediction_distribution(\n",
    "    model=model,\n",
    "    data_loader=val_loader,\n",
    "    idx_to_answer=idx_to_answer,\n",
    "    device=device,\n",
    "    max_samples=1000  # Adjust as needed\n",
    ")\n",
    "\n",
    "# You can also look at specific predictions\n",
    "print(\"\\nSome specific answer frequencies:\")\n",
    "interesting_answers = [\"yes\", \"no\", \"2\", \"red\", \"blue\", \"dog\", \"cat\", \"person\", \"unknown\"]\n",
    "for answer in interesting_answers:\n",
    "    count = prediction_distribution.get(answer, 0)\n",
    "    print(f\"'{answer}': {count} times\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3926c44-7bfe-4865-bfbd-e51998089d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### The above model is not trained well. It's highly overfitted to \"No\" Answers for 940 cases of ~1024 cases. This is expected\n",
    "### as we only trained it for 4 epochs and the Validation accuracy is very low ~20%. We need to train it more to predict\n",
    "### meaningful answers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a4db16-512a-42aa-8828-09b8b31c777a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd51d9b-1e1d-4f5d-89d9-b231fc389abe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61148c2-961c-4d02-b77a-7f172f21b791",
   "metadata": {},
   "outputs": [],
   "source": [
    "### The above code is extremely inefficient, takes a very long time. Let's implement a more effient model. Use more efficience\n",
    "### EfficientNet-B0  and DistilBERT for image and text respectively. Use mixed precision and a fewer layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978d9701-fd6c-48fb-a603-2a70d8143889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified EfficientNet loading code\n",
    "class VQAModel(nn.Module):\n",
    "    def __init__(self, num_answers, hidden_size=512):\n",
    "        super(VQAModel, self).__init__()\n",
    "        \n",
    "        # Image Encoder (EfficientNet-B0 instead of ResNet50)\n",
    "        try:\n",
    "            # First try to import directly from torchvision\n",
    "            from torchvision.models import efficientnet_b0\n",
    "            self.image_encoder = efficientnet_b0(pretrained=True)\n",
    "            # Remove classifier layer\n",
    "            self.image_encoder = nn.Sequential(*list(self.image_encoder.children())[:-1])\n",
    "        except (ImportError, AttributeError):\n",
    "            # Fallback to ResNet50 if EfficientNet isn't available\n",
    "            print(\"EfficientNet not available, falling back to ResNet50\")\n",
    "            resnet = models.resnet50(pretrained=True)\n",
    "            modules = list(resnet.children())[:-1]  # Remove final FC layer\n",
    "            self.image_encoder = nn.Sequential(*modules)\n",
    "            \n",
    "        # Question Encoder (DistilBERT instead of BERT)\n",
    "        from transformers import DistilBertModel\n",
    "        self.question_encoder = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "        \n",
    "        # Projection layers\n",
    "        # Check which backbone we're using to set the right input dimension\n",
    "        if isinstance(self.image_encoder[0], models.resnet.ResNet):\n",
    "            image_features_dim = 2048  # ResNet50\n",
    "        else:\n",
    "            image_features_dim = 1280  # EfficientNet-B0\n",
    "            \n",
    "        self.image_projection = nn.Linear(image_features_dim, hidden_size)\n",
    "        self.question_projection = nn.Linear(768, hidden_size)\n",
    "        \n",
    "        # Cross-attention fusion\n",
    "        self.cross_attention = CrossAttentionFusion(hidden_size)\n",
    "        \n",
    "        # Classification\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, num_answers)\n",
    "        )\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        # Initialize linear layers with Xavier initialization\n",
    "        for module in [self.image_projection, self.question_projection, self.classifier]:\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "    \n",
    "    def forward(self, image, input_ids, attention_mask):\n",
    "        # Use mixed precision where supported\n",
    "        with torch.cuda.amp.autocast(enabled=True):\n",
    "            # Image encoding\n",
    "            batch_size = image.size(0)\n",
    "            image_features = self.image_encoder(image)\n",
    "            if isinstance(image_features, tuple):\n",
    "                image_features = image_features[0]\n",
    "            image_features = image_features.reshape(batch_size, -1)\n",
    "            image_features = self.image_projection(image_features)\n",
    "            \n",
    "            # Question encoding\n",
    "            question_outputs = self.question_encoder(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            question_features = question_outputs.last_hidden_state[:, 0]  # Use first token features\n",
    "            question_features = self.question_projection(question_features)\n",
    "            \n",
    "            # Cross-attention fusion\n",
    "            fused_features = self.cross_attention(image_features, question_features)\n",
    "            \n",
    "            # Classification\n",
    "            output = self.classifier(fused_features)\n",
    "            \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9259e7fc-3697-411f-b4ed-2d85e1394649",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f772f9d9-e6df-4840-8a7a-e4a2e39735f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model and train the model same way as we did earlier.\n",
    "\n",
    "model = VQAModel(num_answers=len(train_dataset.answer_vocab) + 1)  # +1 for unknown answers\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, factor=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64194235-77af-4c6f-93ff-ff4723548c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8dd8198-7065-4bde-a59c-2ffd0f7cf48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop with checkpointing\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=10):\n",
    "    best_val_accuracy = 0.0\n",
    "    t_loss=[]\n",
    "    v_loss=[]\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\"):\n",
    "            # Get data\n",
    "            images = batch['image'].to(device)\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            answer_weights = batch['answer_weights'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images, input_ids, attention_mask)\n",
    "            loss = criterion(outputs, answer_weights)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        t_loss.append(avg_train_loss)\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\"):\n",
    "                # Get data\n",
    "                images = batch['image'].to(device)\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                answer_weights = batch['answer_weights'].to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(images, input_ids, attention_mask)\n",
    "                loss = criterion(outputs, answer_weights)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                # Calculate accuracy (VQA accuracy metric)\n",
    "                pred_indices = torch.argmax(outputs, dim=1)\n",
    "                ground_truth_indices = torch.argmax(answer_weights, dim=1)\n",
    "                correct += (pred_indices == ground_truth_indices).sum().item()\n",
    "                total += images.size(0)\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_accuracy = 100 * correct / total\n",
    "        v_loss.append(avg_val_loss)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"Training Loss: {avg_train_loss:.4f}\")\n",
    "        print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "        print(f\"Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step(avg_val_loss)\n",
    "        # Save model every 5 epochs\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_accuracy': val_accuracy,\n",
    "                'train_loss': avg_train_loss,\n",
    "                'val_loss': avg_val_loss,\n",
    "                'answer_vocab': train_dataset.answer_vocab\n",
    "            }, f'efficient_vqa_model_epoch_{epoch+1}.pth')\n",
    "            print(f\"Saved model checkpoint at epoch {epoch+1}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_accuracy': val_accuracy,\n",
    "                'answer_vocab': train_dataset.answer_vocab\n",
    "            }, 'efficient_best_vqa_model.pth')\n",
    "            print(f\"Saved new best model with accuracy: {val_accuracy:.2f}%\")\n",
    "    \n",
    "    return model, t_loss,v_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46be49e-17b3-4ba0-83eb-827791a6ba7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training\n",
    "model,train_loss,valid_loss = train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe97d859-f3dd-4696-a9c3-74dc7c704158",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train 5 more epochs\n",
    "#model, more_train_loss, more_valid_loss = train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=5)  # Add 6 more epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdfd543-d2fa-4785-bcd7-19d736022075",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The accurcay looks better now. plot losses\n",
    "\n",
    "# train_loss_all=train_loss+more_train_loss # combine from 2 stages of training\n",
    "# valid_loss_all=valid_loss+more_valid_loss\n",
    "\n",
    "# plt.plot(train_loss_all, 'b', label='Training loss')\n",
    "# plt.plot(valid_loss_all, 'r', label='Validation loss')\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_loss, 'b', label='Training loss')\n",
    "plt.plot(valid_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8b01ba-4336-48e4-a160-9aebb753f371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation set\n",
    "val_accuracy = evaluate_model(model, val_loader)\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7680ce47-b3e5-451c-a3ad-b7ecf5203d90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5c8c21-828d-488a-89c0-12c62c4bcb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## How does it perform on the random 5 from the validation set? We do know the right answes here.\n",
    "# Get a few examples from validation set\n",
    "val_examples = []\n",
    "for i, batch in enumerate(val_loader):\n",
    "    if i >= 5:  # Get 5 examples\n",
    "        break\n",
    "    \n",
    "    # Get question_id to find the ground truth\n",
    "    question_id = batch['question_id'][0].item()\n",
    "    \n",
    "    # Find matching annotation for ground truth\n",
    "    ground_truth = \"Unknown\"\n",
    "    for ann in val_dataset.annotations:\n",
    "        if ann['question_id'] == question_id:\n",
    "            # Get most common answer\n",
    "            answers = [a['answer'] for a in ann['answers']]\n",
    "            from collections import Counter\n",
    "            ground_truth = Counter(answers).most_common(1)[0][0]\n",
    "            break\n",
    "    \n",
    "    val_examples.append({\n",
    "        'image': batch['image'][0],\n",
    "        'question_id': question_id,\n",
    "        'question_text': val_dataset.questions[i]['question'],\n",
    "        'ground_truth': ground_truth\n",
    "    })\n",
    "\n",
    "# Run inference on validation examples\n",
    "for i, example in enumerate(val_examples):\n",
    "    # Get image and question\n",
    "    image = example['image'].to(device)\n",
    "    question = example['question_text']\n",
    "    ground_truth = example['ground_truth']\n",
    "    \n",
    "    # Tokenize question\n",
    "    question_tokens = tokenizer(\n",
    "        question,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=32,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    input_ids = question_tokens['input_ids'].to(device)\n",
    "    attention_mask = question_tokens['attention_mask'].to(device)\n",
    "    \n",
    "    # Get prediction\n",
    "    with torch.no_grad():\n",
    "        output = model(image.unsqueeze(0), input_ids, attention_mask)\n",
    "        predicted_idx = torch.argmax(output, dim=1).item()\n",
    "    \n",
    "    # Convert index to answer\n",
    "    if predicted_idx in idx_to_answer:\n",
    "        predicted_answer = idx_to_answer[predicted_idx]\n",
    "    else:\n",
    "        predicted_answer = \"unknown\"\n",
    "    \n",
    "    # Visualize\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(example['image'].permute(1, 2, 0).cpu())\n",
    "    plt.title(f\"Validation Example {i+1}\\nQ: {question}\\nPredicted: {predicted_answer}\\nGround Truth: {ground_truth}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Validation Example {i+1}:\")\n",
    "    print(f\"Question ID: {example['question_id']}\")\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Predicted Answer: {predicted_answer}\")\n",
    "    print(f\"Ground Truth: {ground_truth}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0f2526-bca1-4679-a1a5-cf7871fb6367",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dae35a0-9534-40cd-af84-59278b5f14ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2d169f-697c-4458-8924-d509c0d8977d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1551519c-671a-4eae-9c4b-b8120ad4e8d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c65bcf-c9b4-41aa-a894-be76b2201c53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8bfb69-1437-4f97-93e1-7044e5baa7a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f8f8ab-1751-44f6-8c44-92c45f480405",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67c4753-ae80-4815-a30d-4c074bc83020",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273fe828-2c4b-4cd1-929b-4208dffe014f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c9b35c-d81d-4710-a308-04cb07ad5053",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce46d270-58d2-4378-8a66-5ae22b5d8bf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b57f52-9b44-463f-8c8a-913c4404905d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbf88c6-f4b8-4329-9894-57bda837a278",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a25ad21-a095-40c3-9244-b4650a3ea9ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce315ff-694a-4f4f-8072-7fa3e724d78b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d119c2f-747e-409f-a549-642af2968dd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44403da-06ef-4b2b-a5c0-dc882ff62dc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef71b53-05ed-4b03-bfb9-8a45439dda73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f9e540-0860-4281-9c8a-d82a53a89859",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca2a08f-3f07-457b-a98d-e78384016204",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ab7f53-b843-490f-8faf-6b5ed4e20528",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da503ea9-78c9-4f73-aa0d-6b47cc136b7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aaea9af-7cc2-49c7-af6f-4f46a113c274",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f89d2c-dd8a-4e07-b020-1c877453835f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb06c60-2f07-479b-abbe-912a290080df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec4204d-ba2c-4291-b664-cc6b03ee85de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86964718-f6bb-4a87-a1ef-97652e55b931",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c59a87-b23d-4e95-a54a-8d8c9e45c598",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708d0a6d-f7dd-4a12-afb5-d128913f150c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f450cf27-c742-4cd3-bddd-62d3468c8eaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3ec0df-55f3-419f-9245-2a7a420d6f1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8713d8-4286-42e3-a7c5-3e3b8cfa23cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f844963-cf22-489d-bc61-096bb29c1176",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
